# GPU deployment configuration anchor
x-gpu-deploy: &gpu-deploy
  resources:
    reservations:
      devices:
        - driver: nvidia
          count: all
          capabilities: [gpu]

# Logging configuration anchors
x-logging: &default-logging
  driver: "json-file"
  options:
    max-size: "10m"
    max-file: "5"
    labels: "service,version"

# Centralized logging via Fluent Bit for key ERNI-KI services
# Updated: 2025-08-22 - Optimized configuration for Loki integration
x-fluentd-logging: &fluentd-logging
  driver: "fluentd"
  options:
    # Fluent Bit service address (localhost for Docker bridge network)
    fluentd-address: "localhost:24224"
    # Async sending for performance
    fluentd-async: "true"
    # Retry settings for reliable delivery
    fluentd-retry-wait: "1s"
    fluentd-max-retries: "5"
    # Tag for log source identification
    tag: "docker.{{.Name}}"
    # Additional labels for Loki
    labels: "service,version,environment"

services:
  # Base infrastructure services
  watchtower:
    # Startup command with optimized parameters (schedule is set via env)
    command: --cleanup --label-enable --http-api-update --http-api-metrics
    env_file: env/watchtower.env
    healthcheck:
      test: ["CMD", "/watchtower", "--health-check"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s
    image: containrrr/watchtower:1.7.1
    logging: *default-logging
    restart: unless-stopped
    # Add port for HTTP API
    ports:
      - "8091:8080" # Changed port to avoid conflict
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
    labels:
      # Exclude Watchtower from its own monitoring
      - "com.centurylinklabs.watchtower.enable=false"
      # Label for log identification
      - "com.centurylinklabs.watchtower.scope=infrastructure"
    # Resource limits to prevent overload
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: "0.2"
        reservations:
          memory: 128M
          cpus: "0.1"

  # PostgreSQL database with vector extension (optimized network configuration)
  db:
    depends_on:
      watchtower:
        condition: service_healthy
    env_file: env/db.env
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -d $${POSTGRES_DB} -U $${POSTGRES_USER}"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 20s
    image: pgvector/pgvector:pg15
    logging: *fluentd-logging
    restart: unless-stopped
    volumes:
      - ./data/postgres:/var/lib/postgresql/data
    # Using standard Docker bridge network
    labels:
      # CRITICAL: Disable auto-update for database
      - "com.centurylinklabs.watchtower.enable=false"
      - "com.centurylinklabs.watchtower.monitor-only=true"
      - "com.centurylinklabs.watchtower.scope=critical-database"

  # Redis cache and queues (optimized network configuration)
  redis:
    depends_on:
      watchtower:
        condition: service_healthy
    env_file: env/redis.env
    healthcheck:
      test: ["CMD-SHELL", "redis-cli ping | grep PONG"]
      interval: 30s
      timeout: 3s
      retries: 5
      start_period: 20s
    image: redis:7-alpine
    logging: *fluentd-logging
    restart: unless-stopped
    volumes:
      - ./data/redis:/data
    # Using standard Docker bridge network
    labels:
      # Allow auto-update for Redis
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=cache-services"

  # LiteLLM Context Engineering Gateway (optimized network configuration)
  litellm:
    # Using latest LiteLLM version with thinking tokens support
    image: ghcr.io/berriai/litellm:v1.80.0.rc.1
    container_name: erni-ki-litellm

    # Dependencies - start after critical services
    depends_on:
      db:
        condition: service_healthy
      redis:
        condition: service_healthy
      ollama:
        condition: service_healthy
      watchtower:
        condition: service_healthy

    # Environment file with configuration
    env_file: env/litellm.env

    # Port for API (internal Docker network)
    ports:
      - "4000:4000" # LiteLLM Proxy API

    # Restart policy for production
    restart: unless-stopped

    # Logging configuration
    logging: *fluentd-logging

    # Health check for status monitoring (simplified process check)
    healthcheck:
      test: ["CMD-SHELL", "pgrep -f litellm || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s # Simplified check

    # Volume mounts for configuration
    volumes:
      - ./conf/litellm/config.yaml:/app/config.yaml:ro # Model configuration
      - ./data/litellm:/app/data # Logs and temporary files

    # Command with basic settings
    command: ["--config", "/app/config.yaml", "--port", "4000", "--host", "0.0.0.0"]

    # Extra hosts for connecting to Ollama on host
    extra_hosts:
      - "host.docker.internal:host-gateway"

    # Resource limits for stability (optimized after audit)
    deploy:
      resources:
        limits:
          memory: 6G # Memory limit (increased from 4G to 6G after audit)
          cpus: "1.0" # CPU limit
        reservations:
          memory: 1.5G # Minimum memory (increased from 1G to 1.5G after audit)
          cpus: "0.5" # Minimum CPU

    # Using standard Docker bridge network

    # Labels for monitoring
    labels:
      - "com.erni-ki.service=litellm"
      - "com.erni-ki.version=main-stable"
      - "com.erni-ki.description=Context Engineering Gateway"
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=ai-services"

  # JWT authentication service
  auth:
    build:
      context: ./auth
      dockerfile: Dockerfile
    depends_on:
      watchtower:
        condition: service_healthy
    env_file: env/auth.env
    logging: *fluentd-logging
    ports:
      - "9092:9090" # Changed port to avoid conflict
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "/app/main", "--health-check"]
      interval: 30s
      timeout: 3s
      retries: 3
      start_period: 5s
    labels:
      # Allow auto-update for Auth service
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=auth-services"

  # Cloudflare tunnel for external access
  cloudflared:
    command: tunnel --no-autoupdate run
    depends_on:
      watchtower:
        condition: service_healthy
      nginx:
        condition: service_healthy
    env_file: env/cloudflared.env
    healthcheck:
      disable: true
    image: cloudflare/cloudflared:2024.10.0
    logging: *fluentd-logging
    restart: unless-stopped
    volumes:
      - ./conf/cloudflare/config:/home/nonroot/.cloudflared
    # Using standard Docker bridge network
    labels:
      # Allow auto-update for Cloudflared
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=tunnel-services"

  # Text-to-speech service
  edgetts:
    depends_on:
      watchtower:
        condition: service_healthy
    env_file: env/edgetts.env
    healthcheck:
      test:
        [
          "CMD-SHELL",
          'python3 -c "import socket; s=socket.socket(); s.settimeout(5); s.connect((\"localhost\", 5050)); s.close()" || exit 1',
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    image: travisvn/openai-edge-tts@sha256:4e7e2773350a3296f301b5f66e361daad243bdc4b799eec32613fddcee849040
    logging: *fluentd-logging
    ports:
      - 5050:5050
    restart: unless-stopped
    labels:
      # Allow auto-update for EdgeTTS
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=text-to-speech"

  # Apache Tika file processing service
  tika:
    depends_on:
      watchtower:
        condition: service_healthy
    env_file: env/tika.env
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "timeout 5 bash -c '</dev/tcp/localhost/9998' || exit 1",
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    image: apache/tika@sha256:21d8052de04e491ccf66e8680ade4da6f3d453a56d59f740b4167e54167219b7
    logging: *fluentd-logging
    ports:
      - 9998:9998
    restart: unless-stopped
    labels:
      # Allow auto-update for Tika
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=document-processing"

  # MCPO server for request processing
  mcposerver:
    command: ["--config", "/app/conf/config.json"]
    depends_on:
      watchtower:
        condition: service_healthy
      db:
        condition: service_healthy
    env_file: env/mcposerver.env
    healthcheck:
      test: ["CMD-SHELL", "test -f /proc/1/cmdline"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 15s
    image: ghcr.io/open-webui/mcpo:git-91e8f94
    logging: *fluentd-logging
    ports:
      - "8000:8000"
    restart: unless-stopped
    volumes:
      - ./conf/mcposerver:/app/conf:ro
      - ./data:/app/data
    # Using standard Docker bridge network
    labels:
      # Allow auto-update for MCP Server
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=ai-services"

  # SearXNG search engine (optimized network configuration)
  searxng:
    depends_on:
      redis:
        condition: service_healthy
      watchtower:
        condition: service_healthy
    env_file: env/searxng.env
    healthcheck:
      test:
        [
          "CMD-SHELL",
          'wget -q --spider --header="User-Agent: OpenWebUI-HealthCheck/1.0" --header="X-Real-IP: 127.0.0.1" --header="X-Forwarded-For: 127.0.0.1" --header="Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8" --header="Accept-Language: en-US,en;q=0.5" http://localhost:8080/ || exit 1',
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    image: searxng/searxng@sha256:d477c0460cc06afa57541f24c7adcae3846303a125c3ae785b9893c9c2c2186f
    logging: *fluentd-logging
    restart: unless-stopped
    volumes:
      - ./conf/searxng/settings.yml:/etc/searxng/settings.yml:ro
      - ./conf/searxng/uwsgi.ini:/etc/searxng/uwsgi.ini:ro
      - ./conf/searxng/limiter.toml:/etc/searxng/limiter.toml:ro
      - ./conf/searxng/favicons.toml:/etc/searxng/favicons.toml:ro
    # Using standard Docker bridge network
    labels:
      # Allow auto-update for SearXNG
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=search-services"

  # Ollama LLM server with GPU acceleration (optimized network configuration)
  ollama:
    depends_on:
      watchtower:
        condition: service_healthy
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    env_file: env/ollama.env
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 30s
      timeout: 3s
      retries: 5
      start_period: 10s
    image: ollama/ollama:0.12.11
    logging: *fluentd-logging
    ports:
      - 11434:11434
    restart: unless-stopped
    volumes:
      - ./data/ollama:/root/.ollama
    # Using standard Docker bridge network
    labels:
      # CRITICAL: Disable auto-update for Ollama with GPU
      - "com.centurylinklabs.watchtower.enable=false"
      - "com.centurylinklabs.watchtower.monitor-only=true"
      - "com.centurylinklabs.watchtower.scope=critical-ai-gpu"

  # Nginx reverse proxy (optimized network configuration)
  nginx:
    depends_on:
      auth:
        condition: service_healthy
      openwebui:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl --fail http://localhost/health || exit 1"]
      interval: 30s
      timeout: 3s
      retries: 5
      start_period: 5s
    image: nginx:1.29.3
    logging: *fluentd-logging
    ports:
      - "80:80"
      - "443:443"
      - "8080:8080"
    restart: unless-stopped
    volumes:
      - ./conf/nginx/conf.d/default.conf:/etc/nginx/conf.d/default.conf
      - ./conf/nginx/nginx.conf:/etc/nginx/nginx.conf
      - ./conf/nginx/ssl:/etc/nginx/ssl
    # Using standard Docker bridge network
    labels:
      # CRITICAL: Disable auto-update for Nginx (critical proxy server)
      - "com.centurylinklabs.watchtower.enable=false"
      - "com.centurylinklabs.watchtower.monitor-only=true"
      - "com.centurylinklabs.watchtower.scope=critical-proxy"

  # OpenWebUI main interface with GPU acceleration (optimized network configuration)
  openwebui:
    depends_on:
      auth:
        condition: service_healthy
      db:
        condition: service_healthy
      litellm:
        condition: service_healthy
      ollama:
        condition: service_healthy
      redis:
        condition: service_healthy
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      # MCP Tool Servers через nginx proxy (HTTP endpoints для избежания SSL проблем)
      - 'TOOL_SERVER_CONNECTIONS=[{"name": "Time Server", "url": "http://nginx:8080/api/mcp/time", "enabled": true}, {"name": "PostgreSQL Server", "url": "http://nginx:8080/api/mcp/postgres", "enabled": true}, {"name": "Filesystem Server", "url": "http://nginx:8080/api/mcp/filesystem", "enabled": true}, {"name": "Memory Server", "url": "http://nginx:8080/api/mcp/memory", "enabled": true}]'
    env_file: env/openwebui.env
    healthcheck:
      test: ["CMD-SHELL", "curl --fail http://localhost:8080/health || exit 1"]
      interval: 30s
      timeout: 3s
      retries: 5
      start_period: 10s
    image: ghcr.io/open-webui/open-webui:cuda
    logging: *fluentd-logging
    restart: unless-stopped
    volumes:
      - ./data/openwebui:/app/backend/data
    # Using standard Docker bridge network
    labels:
      # Allow auto-update for OpenWebUI
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=web-interface"

  backrest:
    depends_on:
      - db
      - redis
    env_file: env/backrest.env
    healthcheck:
      test: ["CMD-SHELL", "curl -s http://localhost:9898/ >/dev/null || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s
    image: garethgeorge/backrest:v1.9.2
    logging: *fluentd-logging
    ports:
      - "9898:9898"
    restart: unless-stopped
    volumes:
      - ./data/backrest:/data
      - ./conf/backrest:/config
      - ./cache/backrest:/cache
      - ./tmp/backrest:/tmp
      - ./data:/backup-sources/data:ro
      - ./conf:/backup-sources/conf:ro
      - ./env:/backup-sources/env:ro
      - ./.config-backup:/backup-sources/.config-backup
      - /var/run/docker.sock:/var/run/docker.sock:ro
    labels:
      # Allow auto-update for Backrest
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=backup-services"

  # ============================================================================
  # MONITORING AND LOGGING
  # ============================================================================

  # Prometheus - metrics collection
  prometheus:
    depends_on:
      watchtower:
        condition: service_healthy
    image: prom/prometheus:v2.48.0
    container_name: erni-ki-prometheus
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
      - "--storage.tsdb.retention.time=30d"
      - "--storage.tsdb.retention.size=10GB"
      - "--web.console.libraries=/etc/prometheus/console_libraries"
      - "--web.console.templates=/etc/prometheus/consoles"
      - "--web.enable-lifecycle"
      - "--web.enable-admin-api"
      - "--web.external-url=http://prometheus.erni-ki.local"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./monitoring/alert_rules.yml:/etc/prometheus/alert_rules.yml:ro
      - ./data/prometheus:/prometheus
    ports:
      - "9091:9090"
    restart: unless-stopped
    logging: *default-logging
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "wget --no-verbose --tries=1 --spider http://localhost:9090/-/healthy || exit 1",
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    labels:
      # Allow auto-update for Prometheus
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=monitoring-stack"
    # Using standard Docker bridge network

  # Grafana - dashboard and visualization
  grafana:
    depends_on:
      watchtower:
        condition: service_healthy
      prometheus:
        condition: service_healthy
    image: grafana/grafana:10.2.0
    container_name: erni-ki-grafana
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin123
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_SERVER_ROOT_URL=http://grafana.erni-ki.local
      - GF_SERVER_SERVE_FROM_SUB_PATH=true
      - GF_ANALYTICS_REPORTING_ENABLED=false
      - GF_ANALYTICS_CHECK_FOR_UPDATES=false
    volumes:
      - ./data/grafana:/var/lib/grafana
      # Updated paths for optimized configurations
      - ./conf/grafana/provisioning:/etc/grafana/provisioning:ro
      - ./conf/grafana/dashboards:/var/lib/grafana/dashboards:ro
    ports:
      - "3000:3000"
    restart: unless-stopped
    logging: *default-logging
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:3000/api/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    labels:
      # Allow auto-update for Grafana
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=monitoring-stack"
    # Using standard Docker bridge network

  # Loki - modern logging system (Elasticsearch replacement)
  loki:
    depends_on:
      watchtower:
        condition: service_healthy
    image: grafana/loki:3.4.1
    container_name: erni-ki-loki
    logging: *default-logging
    restart: unless-stopped
    ports:
      - "3100:3100"
    volumes:
      - ./conf/loki/loki-config.yaml:/etc/loki/local-config.yaml:ro
      - ./data/loki:/loki
    command: -config.file=/etc/loki/local-config.yaml
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "wget --header='X-Scope-OrgID: erni-ki' --no-verbose --tries=1 --spider http://localhost:3100/ready || exit 1",
        ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    labels:
      # Allow auto-update for Loki
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=logging-stack"
  # Alertmanager - alert management
  alertmanager:
    depends_on:
      watchtower:
        condition: service_healthy
    image: prom/alertmanager:v0.26.0
    container_name: erni-ki-alertmanager
    command:
      - "--config.file=/etc/alertmanager/alertmanager.yml"
      - "--storage.path=/alertmanager"
      - "--web.external-url=http://alertmanager.erni-ki.local"
      - "--cluster.listen-address=0.0.0.0:9094"
      # Cluster parameters to solve queue overflow issue
      - "--cluster.gossip-interval=500ms" # Increased from default 200ms
      - "--cluster.pushpull-interval=120s" # Increased from default 60s
      - "--cluster.tcp-timeout=15s" # Increased for connection stability
      - "--cluster.probe-timeout=1s" # Optimized for quick diagnostics
      - "--cluster.probe-interval=2s" # Cluster node check interval
    volumes:
      - ./monitoring/alertmanager/alertmanager.yml:/etc/alertmanager/alertmanager.yml:ro
      - ./data/alertmanager:/alertmanager
    ports:
      - "9093:9093"
      - "9094:9094"
    restart: unless-stopped
    logging: *default-logging
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "wget --no-verbose --tries=1 --spider http://localhost:9093/-/healthy || exit 1",
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    labels:
      # Allow auto-update for Alertmanager
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=monitoring-stack"
    secrets:
      - slack_alert_webhook
      - pagerduty_routing_key
    # Using standard Docker bridge network

  # Node Exporter - system metrics
  node-exporter:
    depends_on:
      watchtower:
        condition: service_healthy
    image: prom/node-exporter:v1.7.0
    container_name: erni-ki-node-exporter
    command:
      - "--path.procfs=/host/proc"
      - "--path.rootfs=/rootfs"
      - "--path.sysfs=/host/sys"
      - "--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)"
      - "--collector.systemd"
      - "--collector.processes"
      - "--collector.textfile.directory=/var/lib/node_exporter/textfile_collector"
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
      - ./data/node-exporter-textfile:/var/lib/node_exporter/textfile_collector:ro
      - /run/systemd/private:/run/systemd/private:ro
    ports:
      - "9101:9100"
    pid: host
    restart: unless-stopped
    logging: *default-logging
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "wget --no-verbose --tries=1 --spider http://localhost:9100/metrics || exit 1",
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    labels:
      # Allow auto-update for Node Exporter
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=monitoring-stack"
    # Using standard Docker bridge network

  # Postgres Exporter - PostgreSQL monitoring
  postgres-exporter:
    depends_on:
      watchtower:
        condition: service_healthy
      db:
        condition: service_healthy
    image: prometheuscommunity/postgres-exporter:v0.15.0
    container_name: erni-ki-postgres-exporter
    environment:
      - DATA_SOURCE_NAME=postgresql://postgres:aEnbxS4MrXqzurHNGxkcEgCBm@db:5432/openwebui?sslmode=disable
      - PG_EXPORTER_DISABLE_DEFAULT_METRICS=false
      - PG_EXPORTER_DISABLE_SETTINGS_METRICS=true
      - PG_EXPORTER_AUTO_DISCOVER_DATABASES=true
      - PG_EXPORTER_EXCLUDE_DATABASES=template0,template1,postgres
    volumes:
      - ./conf/postgres-exporter/postgres_exporter.yml:/postgres_exporter.yml:ro
    ports:
      - "9187:9187"
    restart: unless-stopped
    logging: *default-logging
    healthcheck:
      test: ["CMD-SHELL", "wget --quiet --tries=1 --spider http://localhost:9187/metrics || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    labels:
      # Allow auto-update for Postgres Exporter
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=monitoring-stack"
    # Using standard Docker bridge network

  # Redis Exporter - Redis monitoring
  redis-exporter:
    depends_on:
      watchtower:
        condition: service_healthy
      redis:
        condition: service_healthy
    image: oliver006/redis_exporter:v1.55.0
    container_name: erni-ki-redis-exporter
    environment:
      - REDIS_ADDR=redis://redis:6379
      - REDIS_EXPORTER_INCL_SYSTEM_METRICS=true
    ports:
      - "9121:9121"
    restart: unless-stopped
    logging: *default-logging
    healthcheck:
      disable: true # Minimal container without check utilities
    labels:
      # Allow auto-update for Redis Exporter
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=monitoring-stack"
    # Using standard Docker bridge network

  # NVIDIA GPU Exporter - GPU monitoring
  nvidia-exporter:
    depends_on:
      watchtower:
        condition: service_healthy
    image: mindprince/nvidia_gpu_prometheus_exporter:0.1
    container_name: erni-ki-nvidia-exporter
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    ports:
      - "9445:9445"
    restart: unless-stopped
    logging: *default-logging
    healthcheck:
      test: ["CMD-SHELL", "pgrep -f nvidia_gpu_prometheus_exporter > /dev/null || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 15s
    labels:
      # Allow auto-update for NVIDIA Exporter
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=monitoring-stack"
    # Using standard Docker bridge network

  # Blackbox Exporter - availability monitoring
  blackbox-exporter:
    depends_on:
      watchtower:
        condition: service_healthy
    image: prom/blackbox-exporter:v0.24.0
    container_name: erni-ki-blackbox-exporter
    volumes:
      - ./monitoring/blackbox/blackbox.yml:/etc/blackbox_exporter/config.yml:ro
    ports:
      - "9115:9115"
    restart: unless-stopped
    logging: *default-logging
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "wget --no-verbose --tries=1 --spider http://localhost:9115/-/healthy || exit 1",
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    # Using standard Docker bridge network
    labels:
      # Allow auto-update for Blackbox Exporter
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=monitoring-stack"

  # Ollama Exporter - AI services monitoring
  ollama-exporter:
    depends_on:
      watchtower:
        condition: service_healthy
      ollama:
        condition: service_healthy
    build:
      context: ./ops/ollama-exporter
      dockerfile: Dockerfile
    container_name: erni-ki-ollama-exporter
    environment:
      - OLLAMA_URL=http://ollama:11434
      - EXPORTER_PORT=9778
      - LOG_LEVEL=info
    ports:
      - "9778:9778"
    restart: unless-stopped
    logging: *default-logging
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "wget --no-verbose --tries=1 --spider http://127.0.0.1:9778/metrics || exit 1",
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 15s
    labels:
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=monitoring-stack"

  # Nginx Exporter - web server monitoring
  nginx-exporter:
    depends_on:
      watchtower:
        condition: service_healthy
      nginx:
        condition: service_healthy
    image: nginx/nginx-prometheus-exporter:1.1.0
    container_name: erni-ki-nginx-exporter
    command:
      - "-nginx.scrape-uri=http://nginx:8080/nginx_status"
      - "-web.listen-address=:9113"
    ports:
      - "9113:9113"
    restart: unless-stopped
    logging: *default-logging
    # Health check removed - minimal container without curl/wget
    # Metrics available at http://localhost:9113/metrics
    labels:
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=monitoring-stack"

  # cAdvisor - container monitoring
  cadvisor:
    depends_on:
      watchtower:
        condition: service_healthy
    image: gcr.io/cadvisor/cadvisor:v0.47.2
    container_name: erni-ki-cadvisor
    command:
      - "--housekeeping_interval=10s"
      - "--max_housekeeping_interval=15s"
      - "--docker_only=true"
      - "--disable_metrics=disk,network,tcp,udp,percpu,sched,process"
      - "--store_container_labels=false"
      - "--whitelisted_container_labels=io.kubernetes.container.name,io.kubernetes.pod.name"
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
    ports:
      - "8081:8080"
    restart: unless-stopped
    logging: *default-logging
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "wget --no-verbose --tries=1 --spider http://localhost:8080/healthz || exit 1",
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    labels:
      # Allow auto-update for cAdvisor
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=monitoring-stack"
    # Using standard Docker bridge network

  # === CENTRALIZED LOGGING ===

  # Fluent Bit - log collector and processor
  fluent-bit:
    depends_on:
      watchtower:
        condition: service_healthy
      loki:
        condition: service_healthy
    environment:
      # Log level
      - FLB_LOG_LEVEL=info
      # HTTP server for metrics and health checks
      - FLB_HTTP_SERVER=On
      - FLB_HTTP_LISTEN=0.0.0.0
      - FLB_HTTP_PORT=2020
      # Health check endpoint
      - FLB_HEALTH_CHECK=On
    # Health check disabled due to minimal Fluent Bit image
    # Service is monitored via Prometheus metrics on port 2020
    healthcheck:
      disable: true
    image: fluent/fluent-bit:3.0
    container_name: erni-ki-fluent-bit
    logging: *default-logging
    restart: unless-stopped
    volumes:
      # Updated Fluent Bit configuration for Loki integration
      - ./conf/fluent-bit/fluent-bit.conf:/fluent-bit/etc/fluent-bit.conf:ro
      - ./data/logs:/var/log:ro
      - ./data/fluent-bit/db:/fluent-bit/db
      # Docker socket access for container log collection
      - /var/run/docker.sock:/var/run/docker.sock:ro
    ports:
      - "2020:2020" # HTTP API и метрики
      - "24224:24224" # Fluentd forward protocol
    labels:
      # Allow auto-update for Fluent Bit
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=logging-stack"

  # Webhook Receiver - alert processing from Alertmanager
  webhook-receiver:
    depends_on:
      watchtower:
        condition: service_healthy
      alertmanager:
        condition: service_healthy
    build:
      context: ./monitoring/webhook-receiver
      dockerfile: Dockerfile
    container_name: erni-ki-webhook-receiver
    environment:
      # Port for webhook endpoints
      - WEBHOOK_PORT=9093
      # Python settings
      - PYTHONUNBUFFERED=1
      - FLASK_ENV=production
      # Logging settings
      - LOG_LEVEL=INFO
    volumes:
      # Webhook receiver logs
      - ./monitoring/logs/webhook:/app/logs
      # Scripts for alert processing
      - ./monitoring/webhook-receiver/scripts:/app/scripts:ro
    ports:
      - "9095:9093" # Webhook endpoint (avoiding conflict with Alertmanager 9093)
    restart: unless-stopped
    logging: *default-logging
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9093/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: "0.2"
        reservations:
          memory: 128M
          cpus: "0.1"
    labels:
      # Allow auto-update for Webhook Receiver
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=monitoring-stack"
# Using standard Docker bridge network (docker0)

secrets:
  slack_alert_webhook:
    file: ./secrets/slack_alert_webhook.txt
  pagerduty_routing_key:
    file: ./secrets/pagerduty_routing_key.txt
# Using standard Docker bridge network (docker0)
