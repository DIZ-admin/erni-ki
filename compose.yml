# ============================================================================
# ERNI-KI 4-TIER LOGGING STRATEGY
# ============================================================================

# TIER 1: Critical Services (OpenWebUI, Ollama, PostgreSQL, Nginx)
# Dual logging: fluentd + json-file backup, maximum reliability
x-critical-logging: &critical-logging
  driver: "json-file"
  options:
    max-size: "50m"
    max-file: "10"
    labels: "service,version,environment,level=critical"
    tag: "critical.{{.Name}}"

# TIER 2: Important Services (SearXNG, Redis, Backrest, Auth, Cloudflared)
# Fluentd with buffering, standard reliability
x-important-logging: &important-logging
  driver: "json-file"
  options:
    max-size: "10m"
    max-file: "5"
    labels: "service,version,environment,level=important"

# TIER 3: Auxiliary Services (Docling, EdgeTTS, Tika, MCP)
# Fluentd with separate tags + tail fallback
x-auxiliary-logging: &auxiliary-logging
  driver: "json-file"
  options:
    max-size: "10m"
    max-file: "5"
    labels: "service,version,level=auxiliary"

# TIER 4: Monitoring Services (Prometheus, Grafana, Exporters)
# Minimal Fluentd logging with aggressive filtering
x-monitoring-logging: &monitoring-logging
  driver: "json-file"
  options:
    max-size: "10m"
    max-file: "5"
    labels: "service,version,level=monitoring"

x-logging: &default-logging
  driver: "json-file"
  options:
    max-size: "10m"
    max-file: "5"
    labels: "service,version"

services:
  watchtower:
    command:
      - --cleanup
      - --label-enable
      - --http-api-update
      - --http-api-metrics
    entrypoint:
      - /opt/erni/bin/busybox
      - sh
      - /watchtower-entrypoint.sh
    env_file: env/watchtower.env
    environment:
      DOCKER_API_VERSION: "1.44"
    healthcheck:
      test: ["CMD", "/watchtower", "--health-check"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s
    image: containrrr/watchtower:1.7.1 # Pinned version for production (latest stable)
    logging: *default-logging
    restart: unless-stopped
    mem_limit: 256m
    mem_reservation: 128m
    cpus: "0.2"
    oom_score_adj: 500
    security_opt:
      - no-new-privileges:true
    # Run as root to guarantee access to docker.sock on hosts where docker group GID differs
    user: "0:0"
    group_add:
      - "125"
    # Port for HTTP API
    ports:
      - "127.0.0.1:8091:8080" # Localhost-only access for HTTP API
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - ./scripts/entrypoints/watchtower.sh:/watchtower-entrypoint.sh:ro
      - ./scripts/entrypoints/busybox:/opt/erni/bin/busybox:ro
    secrets:
      - watchtower_api_token
    networks:
      - monitoring
    labels:
      # Exclude Watchtower from self-monitoring
      - "com.centurylinklabs.watchtower.enable=false"
      # Label for log identification
      - "com.centurylinklabs.watchtower.scope=infrastructure"

  db:
    env_file: env/db.env
    secrets:
      - postgres_password
    environment:
      # Override POSTGRES_PASSWORD from env file to use secret
      POSTGRES_PASSWORD_FILE: /run/secrets/postgres_password
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -d $${POSTGRES_DB} -U $${POSTGRES_USER}"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 20s
    image: pgvector/pgvector:pg17
    logging: *critical-logging
    restart: unless-stopped
    volumes:
      - ./data/postgres:/var/lib/postgresql/data
      - ./conf/postgres-enhanced/postgresql.conf:/etc/postgresql/postgresql.conf:ro
      - ./secrets/postgres-ssl/server.crt:/etc/ssl/postgres/server.crt:ro
      - ./secrets/postgres-ssl/server.key:/etc/ssl/postgres/server.key:ro
    # Using custom configuration
    command: postgres -c config_file=/etc/postgresql/postgresql.conf
    networks:
      - data
    labels:
      # CRITICAL: Disable database auto-updates
      - "com.centurylinklabs.watchtower.enable=false"
      - "com.centurylinklabs.watchtower.monitor-only=true"
      - "com.centurylinklabs.watchtower.scope=critical-database"
    mem_limit: 4g
    mem_reservation: 2g
    cpus: "2.0"

  redis:
    env_file: env/redis.env
    environment:
      REDIS_PASSWORD: "${REDIS_PASSWORD:-}"
    command:
      - /bin/sh
      - -c
      - >
        set -euo pipefail;
        SECRET="$$(tr -d '\r\n' < /run/secrets/redis_password)";
        CONF_TPL=/usr/local/etc/redis/redis.conf;
        CONF_OUT=/tmp/redis.conf;
        ACL_TPL=/usr/local/etc/redis/users.acl.template;
        ACL_OUT=/tmp/redis-users.acl;
        sed "s/\$${REDIS_PASSWORD}/$${SECRET}/g" "$${CONF_TPL}" > "$${CONF_OUT}";
        if [ -f "$${ACL_TPL}" ]; then
          sed "s/\$${REDIS_PASSWORD}/$${SECRET}/g" "$${ACL_TPL}" > "$${ACL_OUT}";
          chmod 600 "$${ACL_OUT}";
          exec redis-server "$${CONF_OUT}" --aclfile "$${ACL_OUT}";
        else
          exec redis-server "$${CONF_OUT}";
        fi
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "pw=$(tr -d '\r\n' < /run/secrets/redis_password); redis-cli -a \"$$pw\" ping | grep PONG",
        ]
      interval: 30s
      timeout: 3s
      retries: 5
      start_period: 20s
    image: redis:7.0.15-alpine # Pinned: avoid floating latest; 7.2 RDB format incompatible
    logging: *important-logging
    restart: unless-stopped
    volumes:
      - ./data/redis:/data
      - ./conf/redis/redis.conf:/usr/local/etc/redis/redis.conf:ro
      - ./conf/redis/users.acl:/usr/local/etc/redis/users.acl.template:ro
    secrets:
      - redis_password
    networks:
      - data
    labels:
      # Allow auto-updates for Redis
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=cache-services"
    mem_limit: 4g
    mem_reservation: 2g
    cpus: "1.0"

  litellm:
    image: ghcr.io/berriai/litellm:v1.80.0-stable.1
    container_name: erni-ki-litellm

    depends_on:
      db:
        condition: service_healthy
      ollama:
        condition: service_healthy
    env_file: env/litellm.env
    environment:
      # DATABASE_URL must be set before Prisma migration runs
      DATABASE_URL: postgresql://litellm_user:LL_secure_pass_2025!@db:5432/litellm?sslmode=disable
    entrypoint:
      - /bin/bash
      - /opt/erni/bin/litellm-entrypoint.sh

    ports:
      - "127.0.0.1:4000:4000"
    expose:
      - "9109"
    restart: unless-stopped
    logging: *auxiliary-logging
    # Health check: using Python urllib (curl not installed in LiteLLM container)
    healthcheck:
      test:
        [
          "CMD-SHELL",
          'python3 -c "import urllib.request; urllib.request.urlopen(''http://localhost:4000/health/liveliness'', timeout=5).read()" || exit 1',
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    volumes:
      - ./conf/litellm/config.yaml:/app/config.yaml:ro
      - ./data/litellm:/app/data
      - ./scripts/entrypoints/litellm.sh:/opt/erni/bin/litellm-entrypoint.sh:ro
      - ./conf/litellm/custom_providers:/app/custom_providers:ro
    command: ["--config", "/app/config.yaml", "--port", "4000", "--host", "0.0.0.0"]
    extra_hosts:
      - "host.docker.internal:host-gateway"

    mem_limit: 12g
    mem_reservation: 6g
    cpus: "1.0"
    oom_score_adj: -300
    networks:
      - backend
      - data
    labels:
      - "com.erni-ki.service=litellm"
      - "com.erni-ki.version=main-stable"
      - "com.erni-ki.description=Context Engineering Gateway"
      - "com.centurylinklabs.watchtower.enable=false"
      - "com.centurylinklabs.watchtower.monitor-only=true"
      - "com.centurylinklabs.watchtower.scope=ai-services"
    secrets:
      - litellm_db_password
      - litellm_api_key
      - litellm_master_key
      - litellm_salt_key
      - litellm_ui_password
      - openai_api_key
      - publicai_api_key
      - redis_password

  auth:
    build:
      context: ./auth
      dockerfile: Dockerfile
    env_file: env/auth.env
    logging: *important-logging
    networks:
      - backend
      - data
    restart: unless-stopped
    mem_limit: 512m
    mem_reservation: 256m
    cpus: "0.5"
    healthcheck:
      test: ["CMD", "/app/main", "--health-check"]
      interval: 30s
      timeout: 3s
      retries: 3
      start_period: 5s
    labels:
      # Disable Watchtower for locally-built image (no registry)
      - "com.centurylinklabs.watchtower.enable=false"
      - "com.centurylinklabs.watchtower.scope=auth-services"
    environment:
      - WEBUI_SECRET_KEY_FILE=/run/secrets/openwebui_secret_key
    secrets:
      - openwebui_secret_key

  cloudflared:
    entrypoint: ["/opt/erni/bin/busybox", "sh", "/opt/erni/bin/cloudflared-with-secret.sh"]
    command: ["tunnel", "--no-autoupdate", "run"]
    depends_on:
      nginx:
        condition: service_healthy
      openwebui:
        condition: service_healthy
    env_file: env/cloudflared.env
    # TUNNEL_ORIGIN_CERT is not required for token-based (remote-managed) tunnels
    # The certificate is only needed for locally-managed tunnels
    healthcheck:
      test: ["CMD", "cloudflared", "version"]
      interval: 60s
      timeout: 15s
      retries: 3
      start_period: 30s
    image: cloudflare/cloudflared:2025.11.1 # Updated: 2025-11-27 to 2025.11.1
    logging: *important-logging
    restart: unless-stopped
    mem_limit: 256m
    mem_reservation: 128m
    cpus: "0.25"
    volumes:
      - ./conf/cloudflare/config:/home/nonroot/.cloudflared
      - ./scripts/entrypoints/cloudflared-with-secret.sh:/opt/erni/bin/cloudflared-with-secret.sh:ro
      - ./scripts/entrypoints/busybox:/opt/erni/bin/busybox:ro
    secrets:
      - source: cloudflared_tunnel_token
        uid: "65532"
        gid: "65532"
        mode: 0444 # allow read for healthcheck/entrypoint even if uid mapping differs
    networks:
      - frontend
      - backend
    labels:
      # Allow auto-updates for Cloudflared
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=tunnel-services"

  edgetts:
    env_file: env/edgetts.env
    healthcheck:
      test:
        [
          "CMD-SHELL",
          'python3 -c "import socket; s=socket.socket(); s.settimeout(5); s.connect((\"localhost\", 5050)); s.close()" || exit 1',
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    image: travisvn/openai-edge-tts@sha256:4e7e2773350a3296f301b5f66e361daad243bdc4b799eec32613fddcee849040 # updated: digest compatible with schema2
    logging: *auxiliary-logging
    networks:
      - backend
    restart: unless-stopped
    secrets:
      - edgetts_api_key
    mem_limit: 1g
    mem_reservation: 512m
    cpus: "0.5"
    labels:
      # Allow auto-updates for EdgeTTS
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=text-to-speech"

  tika:
    env_file: env/tika.env
    healthcheck:
      test: ["CMD-SHELL", "timeout 5 bash -c '</dev/tcp/localhost/9998' || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    image: apache/tika:3.2.3.0-full # updated tag (was digest)
    logging: *auxiliary-logging
    networks:
      - backend
    restart: unless-stopped
    mem_limit: 1g
    mem_reservation: 512m
    cpus: "0.5"
    labels:
      # Allow auto-updates for Tika
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=document-processing"

  mcposerver:
    entrypoint: ["/opt/erni/bin/mcposerver-with-secrets.sh"]
    command:
      - --config
      - /app/conf/config.json
      - --server-type
      - http
      - --host
      - 0.0.0.0
      - --port
      - "8000"
    depends_on:
      db:
        condition: service_healthy
    env_file: env/mcposerver.env
    healthcheck:
      test:
        [
          "CMD-SHELL",
          'python3 -c "import urllib.request; urllib.request.urlopen(''http://localhost:8000/docs'', timeout=5).read()" || exit 1',
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    image: ghcr.io/open-webui/mcpo:git-91e8f94 # v0.0.19 tag not available in registry; keep stable commit
    logging: *auxiliary-logging
    networks:
      - backend
      - data
    restart: unless-stopped
    mem_limit: 512m
    mem_reservation: 256m
    cpus: "0.5"
    volumes:
      - ./conf/mcposerver:/app/conf:ro
      - ./data:/app/data
      - ./scripts/entrypoints/mcposerver-with-secrets.sh:/opt/erni/bin/mcposerver-with-secrets.sh:ro
      - ./scripts/entrypoints/busybox:/opt/erni/bin/busybox:ro
    secrets:
      - postgres_password
      - context7_api_key
      - ragflow_api_key
    labels:
      # Allow auto-updates for MCP Server
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=ai-services"

  searxng:
    depends_on:
      redis:
        condition: service_healthy
    env_file: env/searxng.env
    entrypoint:
      - /opt/erni/bin/searxng-entrypoint.sh
    healthcheck:
      test:
        [
          "CMD-SHELL",
          'wget -q --spider --header="User-Agent: OpenWebUI-HealthCheck/1.0" --header="X-Real-IP: 127.0.0.1" --header="X-Forwarded-For: 127.0.0.1" --header="Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8" --header="Accept-Language: en-US,en;q=0.5" http://localhost:8080/ || exit 1',
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    image: searxng/searxng:2025.11.21-b876d0bed # pinned to avoid surprises from :latest
    logging: *important-logging
    restart: unless-stopped
    volumes:
      - ./conf/searxng/settings.yml:/etc/searxng/settings.yml:ro
      - ./conf/searxng/uwsgi.ini:/etc/searxng/uwsgi.ini:ro
      - ./conf/searxng/limiter.toml:/etc/searxng/limiter.toml:ro
      - ./conf/searxng/favicons.toml:/etc/searxng/favicons.toml:ro
      - ./scripts/entrypoints/searxng.sh:/opt/erni/bin/searxng-entrypoint.sh:ro
    secrets:
      - redis_password
      - searxng_secret_key
    networks:
      - backend
      - data
    labels:
      # Allow auto-updates for SearXNG
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=search-services"
    mem_limit: 1g
    mem_reservation: 512m
    cpus: "1.0"

  ollama:
    runtime: nvidia
    mem_limit: 24g
    mem_reservation: 12g
    cpus: "12.0"
    oom_score_adj: -900
    environment:
      - NVIDIA_VISIBLE_DEVICES=${OLLAMA_GPU_VISIBLE_DEVICES:-0}
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - CUDA_VISIBLE_DEVICES=${OLLAMA_CUDA_VISIBLE_DEVICES:-0}
      - OLLAMA_NUM_PARALLEL=1
      - OLLAMA_MAX_LOADED_MODELS=1
      - OLLAMA_NUM_CTX=2048
      # Flash attention occasionally breaks some models; disable for stability
      - OLLAMA_FLASH_ATTENTION=0
      - CUDA_DEVICE_ORDER=PCI_BUS_ID
    env_file: env/ollama.env
    # GPU memory limits optimized for RTX 5000 (16GB VRAM)
    # Ollama gets GPU priority (VRAM management via OLLAMA_MAX_VRAM=0.75)
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 30s
      timeout: 3s
      retries: 5
      start_period: 10s
    image: ollama/ollama:0.13.0 # Updated to 0.13.0 (GPU/Vulkan improvements)
    logging: *critical-logging
    networks:
      - backend
    restart: unless-stopped
    volumes:
      - ./data/ollama:/root/.ollama
    labels:
      # CRITICAL: Disable Ollama auto-updates (GPU)
      - "com.centurylinklabs.watchtower.enable=false"
      - "com.centurylinklabs.watchtower.monitor-only=true"
      - "com.centurylinklabs.watchtower.scope=critical-ai-gpu"

  nginx:
    depends_on:
      auth:
        condition: service_healthy
      openwebui:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "/etc/nginx/healthcheck.sh"]
      interval: 60s
      timeout: 30s
      retries: 3
      start_period: 30s
    image: nginx:1.29.3
    logging: *critical-logging
    ports:
      - "80:80"
      - "443:443"
      - "8080:8080"
    restart: unless-stopped
    mem_reservation: 384m
    volumes:
      - ./conf/nginx/conf.d/default.conf:/etc/nginx/conf.d/default.conf
      - ./conf/nginx/nginx.conf:/etc/nginx/nginx.conf
      - ./conf/nginx/ssl:/etc/nginx/ssl
      - ./conf/nginx/includes:/etc/nginx/includes
      - ./conf/nginx/healthcheck.sh:/etc/nginx/healthcheck.sh:ro
      - ./data/nginx/webroot:/var/www/html
      - ./data/nginx/logs:/var/log/nginx
    networks:
      - frontend
      - backend
      - monitoring
    labels:
      # CRITICAL: Disable Nginx auto-updates (critical proxy server)
      - "com.centurylinklabs.watchtower.enable=false"
      - "com.centurylinklabs.watchtower.monitor-only=true"
      - "com.centurylinklabs.watchtower.scope=critical-proxy"
    mem_limit: 512m
    cpus: "1.0"
    security_opt:
      - no-new-privileges:true

  openwebui:
    depends_on:
      auth:
        condition: service_healthy
      db:
        condition: service_healthy
      litellm:
        condition: service_healthy
      ollama:
        condition: service_healthy
      redis:
        condition: service_healthy
    image: ghcr.io/open-webui/open-webui:v0.6.40 # Updated: 2025-11-28 to v0.6.40 (latest stable patch)
    runtime: nvidia
    mem_limit: 8g
    mem_reservation: 4g
    cpus: "4.0"
    oom_score_adj: -600
    environment:
      - NVIDIA_VISIBLE_DEVICES=${OPENWEBUI_GPU_VISIBLE_DEVICES:-0}
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - CUDA_VISIBLE_DEVICES=${OPENWEBUI_CUDA_VISIBLE_DEVICES:-0}
    env_file: env/openwebui.env
    entrypoint:
      - /bin/bash
      - /opt/erni/bin/openwebui-entrypoint.sh
    healthcheck:
      test: ["CMD-SHELL", "curl --fail http://localhost:8080/health || exit 1"]
      interval: 30s
      timeout: 3s
      retries: 5
      start_period: 10s
    logging: *critical-logging
    restart: unless-stopped
    volumes:
      - ./data/openwebui:/app/backend/data
      - ./data/docling/shared:/app/backend/data/docling-shared
      - ./scripts/entrypoints/openwebui.sh:/opt/erni/bin/openwebui-entrypoint.sh:ro
    secrets:
      - postgres_password
      - litellm_api_key
      - openwebui_secret_key
      - redis_password
    networks:
      - backend
      - data
    labels:
      # Allow auto-updates for OpenWebUI
      - "com.centurylinklabs.watchtower.enable=false"
      - "com.centurylinklabs.watchtower.monitor-only=true"
      - "com.centurylinklabs.watchtower.scope=web-interface"

  docling:
    # Override DOCLING_IMAGE in .env with pinned digest/tag
    image: ${DOCLING_IMAGE:-ghcr.io/docling-project/docling-serve-cu126@sha256:2db21d1dfe3788fa13af51adc5dd2e650ca66277800ce55bbee3e36dc2533171}
    depends_on:
      ollama:
        condition: service_healthy
      redis:
        condition: service_healthy
    runtime: nvidia
    secrets:
      - docling_picture_api
    mem_limit: 12g
    mem_reservation: 8g
    cpus: "8.0"
    oom_score_adj: -500
    env_file: env/docling.env
    environment:
      - NVIDIA_VISIBLE_DEVICES=${DOCLING_GPU_VISIBLE_DEVICES:-0}
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - CUDA_VISIBLE_DEVICES=${DOCLING_CUDA_VISIBLE_DEVICES:-0}
      - CUDA_DEVICE_ORDER=PCI_BUS_ID
    healthcheck:
      test: ["CMD-SHELL", "curl --fail http://localhost:5001/health || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s
    logging: *auxiliary-logging
    restart: unless-stopped
    volumes:
      - ./data/docling/models:/opt/app-root/src/.cache/huggingface
      - ./data/docling/easyocr:/opt/app-root/src/.EasyOCR
      - ./data/docling/docling-models:/opt/app-root/src/.cache/docling
      - ./data/docling/docling-models:/docling-artifacts
      - ./data/docling/shared:/docling-shared
    networks:
      - backend
      - data
    labels:
      - "com.centurylinklabs.watchtower.enable=false"
      - "com.centurylinklabs.watchtower.scope=document-processing"
      - "com.erni-ki.service=docling"

  backrest:
    depends_on:
      - db
      - redis
    env_file: env/backrest.env
    environment:
      RESTIC_PASSWORD_FILE: /run/secrets/restic_password
    healthcheck:
      test: ["CMD-SHELL", "curl -s http://localhost:9898/ >/dev/null || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s
    image: garethgeorge/backrest:v1.10.1
    logging: *important-logging
    networks:
      - data
    restart: unless-stopped
    mem_limit: 512m
    mem_reservation: 256m
    cpus: "0.5"
    volumes:
      - ./data/backrest:/data
      - ./conf/backrest:/config
      - ./cache/backrest:/cache
      - ./tmp/backrest:/tmp
      - ./data:/backup-sources/data:ro
      - ./conf:/backup-sources/conf:ro
      - ./env:/backup-sources/env:ro
      - ./.config-backup:/backup-sources/.config-backup
      - /var/run/docker.sock:/var/run/docker.sock:ro
    secrets:
      - restic_password
    labels:
      # Allow auto-updates for Backrest
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=backup-services"

  # ============================================================================
  # MONITORING AND LOGGING
  # ============================================================================

  prometheus:
    image: prom/prometheus:v3.7.3 # Updated from v3.0.0
    container_name: erni-ki-prometheus
    depends_on:
      postgres-exporter:
        condition: service_healthy
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
      - "--storage.tsdb.retention.time=30d"
      - "--storage.tsdb.retention.size=10GB"
      - "--web.console.libraries=/etc/prometheus/console_libraries"
      - "--web.console.templates=/etc/prometheus/consoles"
      - "--web.external-url=http://prometheus.erni-ki.local"
    volumes:
      - ./conf/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./conf/prometheus/alert_rules.yml:/etc/prometheus/alert_rules.yml:ro
      - ./conf/prometheus/alerts.yml:/etc/prometheus/alerts.yml:ro
      - ./conf/prometheus/rules:/etc/prometheus/rules:ro
      - ./conf/prometheus/alerts:/etc/prometheus/alerts:ro
      - ./data/prometheus:/prometheus
    ports:
      - "127.0.0.1:9091:9090"
    restart: unless-stopped
    mem_limit: 2g
    mem_reservation: 1g
    cpus: "1.5"
    logging: *monitoring-logging
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "wget --no-verbose --tries=1 --spider http://localhost:9090/-/healthy || exit 1",
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    labels:
      # Allow auto-updates for Prometheus
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=monitoring-stack"
    networks:
      - monitoring

  grafana:
    depends_on:
      prometheus:
        condition: service_healthy
    image: grafana/grafana:12.3.0
    container_name: erni-ki-grafana
    user: "0:0"
    env_file: env/grafana.env
    environment:
      GF_SECURITY_ADMIN_PASSWORD__FILE: /run/secrets/grafana_admin_password
      GF_PATHS_DATA: "/var/lib/grafana"
      GF_PATHS_LOGS: "/var/log/grafana"
      GF_PATHS_PLUGINS: "/var/lib/grafana/plugins"
      GF_PATHS_PROVISIONING: "/etc/grafana/provisioning"
      GF_SERVER_ROOT_URL: "http://grafana.erni-ki.local"
      GF_SERVER_SERVE_FROM_SUB_PATH: true
      GF_ANALYTICS_REPORTING_ENABLED: false
      GF_ANALYTICS_CHECK_FOR_UPDATES: false
    volumes:
      - ./data/grafana:/var/lib/grafana
      - ./conf/grafana/provisioning:/etc/grafana/provisioning:ro
      - ./conf/grafana/dashboards:/var/lib/grafana/dashboards:ro
    ports:
      - "127.0.0.1:3000:3000"
    restart: unless-stopped
    mem_limit: 1g
    mem_reservation: 512m
    cpus: "0.5"
    logging: *monitoring-logging
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:3000/api/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    labels:
      # Allow auto-updates for Grafana
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=monitoring-stack"
    secrets:
      - grafana_admin_password
    networks:
      - monitoring

  uptime-kuma:
    image: louislam/uptime-kuma:2.0.2
    container_name: erni-ki-uptime-kuma
    env_file: env/uptime-kuma.env
    restart: unless-stopped
    logging: *monitoring-logging
    ports:
      - "127.0.0.1:3001:3001" # Localhost-only
    volumes:
      - ./data/uptime-kuma:/app/data
    healthcheck:
      test: ["CMD", "node", "/app/extra/healthcheck.js"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    mem_limit: 512m
    mem_reservation: 256m
    cpus: "0.5"
    labels:
      # Allow auto-updates for Uptime Kuma
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=monitoring-stack"
    networks:
      monitoring:
        aliases:
          - uptime-kuma
      frontend:
        aliases:
          - uptime-kuma
      # Added backend and data networks to resolve DNS for internal services
      backend:
        aliases:
          - uptime-kuma
      data:
        aliases:
          - uptime-kuma

  loki:
    image: grafana/loki:3.6.2 # Updated from 3.0.0
    container_name: erni-ki-loki
    logging: *monitoring-logging
    restart: unless-stopped
    ports:
      - "127.0.0.1:3100:3100"
    mem_limit: 2g
    mem_reservation: 1g
    cpus: "1.0"
    volumes:
      - ./conf/loki/loki-config.yaml:/etc/loki/local-config.yaml:ro
      - ./data/loki:/loki
      - ./conf/loki/tls:/etc/loki/tls:ro
    command:
      - -config.file=/etc/loki/local-config.yaml
      - -config.expand-env=true
    healthcheck:
      test: ["CMD", "/usr/bin/loki", "--version"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    labels:
      # Allow auto-updates for Loki
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=logging-stack"
    networks:
      - monitoring

  alertmanager:
    image: prom/alertmanager:v0.29.0 # updated from v0.27.0
    container_name: erni-ki-alertmanager
    command:
      - "--config.file=/etc/alertmanager/alertmanager.yml"
      - "--storage.path=/alertmanager"
      - "--web.external-url=http://alertmanager.erni-ki.local"
      - "--cluster.listen-address=0.0.0.0:9094"
      # Cluster parameters to resolve queue overflow issue
      - "--cluster.gossip-interval=500ms" # Increased from default 200ms
      - "--cluster.pushpull-interval=120s" # Increased from default 60s
      - "--cluster.tcp-timeout=15s" # Increased for connection stability
      - "--cluster.probe-timeout=1s" # Optimized for fast diagnostics
      - "--cluster.probe-interval=2s" # Cluster node check interval
    volumes:
      - ./conf/alertmanager/alertmanager.yml:/etc/alertmanager/alertmanager.yml:ro
      - ./data/alertmanager:/alertmanager
    ports:
      - "127.0.0.1:9093:9093"
      - "127.0.0.1:9094:9094"
    restart: unless-stopped
    mem_limit: 512m
    mem_reservation: 256m
    cpus: "0.5"
    logging: *monitoring-logging
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "wget --no-verbose --tries=1 --spider http://localhost:9093/-/healthy || exit 1",
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    labels:
      # Allow auto-updates for Alertmanager
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=monitoring-stack"
    secrets:
      - source: slack_alert_webhook
        target: slack_alert_webhook
        mode: 0444
      - source: pagerduty_routing_key
        target: pagerduty_routing_key
        mode: 0444
    networks:
      - monitoring

  node-exporter:
    image: prom/node-exporter:v1.10.2 # updated from v1.8.2
    container_name: erni-ki-node-exporter
    command:
      - "--path.procfs=/host/proc"
      - "--path.rootfs=/rootfs"
      - "--path.sysfs=/host/sys"
      - "--path.udev.data=/host/run/udev/data" # Path to udev data for diskstats collector
      - "--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)"
      - "--no-collector.systemd" # Disable systemd collector (no dbus access in container)
      # FIXED 2025-10-24: error instead of warn to suppress broken pipe errors
      # Broken pipe errors are normal behavior when client closes connection
      - "--log.level=error"
      - "--collector.processes"
      - "--web.disable-exporter-metrics" # Disable exporter's own metrics to reduce load
      - "--collector.textfile.directory=/var/lib/node_exporter/textfile_collector"
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
      - /run/udev/data:/host/run/udev/data:ro # Mount udev data for diskstats
      - /run/systemd/private:/run/systemd/private:ro
      - ./data/node-exporter-textfile:/var/lib/node_exporter/textfile_collector:ro
    ports:
      - "127.0.0.1:9101:9100"
    pid: host
    restart: unless-stopped
    mem_limit: 256m
    mem_reservation: 128m
    cpus: "0.2"
    logging: *monitoring-logging
    security_opt:
      - no-new-privileges:true
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://localhost:9100/metrics >/dev/null"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    labels:
      # Allow auto-updates for Node Exporter
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=monitoring-stack"
    networks:
      - monitoring

  postgres-exporter:
    depends_on:
      db:
        condition: service_healthy
    image: prometheuscommunity/postgres-exporter:v0.18.1
    container_name: erni-ki-postgres-exporter
    user: "0:0"
    environment:
      - PG_EXPORTER_DISABLE_DEFAULT_METRICS=false
      - PG_EXPORTER_DISABLE_SETTINGS_METRICS=true
      - PG_EXPORTER_AUTO_DISCOVER_DATABASES=true
      - PG_EXPORTER_EXCLUDE_DATABASES=template0,template1,postgres
    volumes:
      - ./scripts/infrastructure/postgres-exporter-entrypoint.sh:/entrypoint/postgres-exporter.sh:ro
      - ./conf/postgres-exporter/config.yml:/etc/postgres_exporter.yml:ro
    ports:
      - "127.0.0.1:9188:9188"
    restart: unless-stopped
    mem_limit: 256m
    mem_reservation: 128m
    cpus: "0.2"
    entrypoint:
      - /entrypoint/postgres-exporter.sh
    command:
      - --config.file=/etc/postgres_exporter.yml
      - --no-collector.stat_bgwriter
    logging: *monitoring-logging
    security_opt:
      - no-new-privileges:true
    healthcheck:
      test: ["CMD-SHELL", "wget --quiet --tries=1 --spider http://localhost:9187/metrics || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    labels:
      # Allow auto-updates for Postgres Exporter
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=monitoring-stack"
    secrets:
      - source: postgres_exporter_dsn
        target: /etc/postgres_exporter_dsn.txt
        mode: 0444
    networks:
      - monitoring
      - data

  # Socat Proxy: Postgres Exporter listens on IPv6, Prometheus connects via IPv4
  postgres-exporter-proxy:
    depends_on:
      postgres-exporter:
        condition: service_healthy
    image: alpine/socat@sha256:86b69d2e491f6c32a9ce5ec8e3489c195ba314a0f99ca30debf6007376e795e5
    container_name: erni-ki-postgres-exporter-proxy
    network_mode: "service:postgres-exporter"
    entrypoint: ["socat"]
    command: ["TCP4-LISTEN:9188,fork,reuseaddr", "TCP6:[::1]:9187"]
    restart: unless-stopped
    mem_limit: 256m
    mem_reservation: 128m
    cpus: "0.2"
    logging: *monitoring-logging
    labels:
      - "com.centurylinklabs.watchtower.enable=false"
      - "com.centurylinklabs.watchtower.scope=monitoring-stack"
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "/opt/erni/bin/busybox wget -q --tries=1 --spider http://127.0.0.1:9188/metrics || exit 1",
        ]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s
    volumes:
      - ./scripts/entrypoints/busybox:/opt/erni/bin/busybox:ro

  # NVIDIA GPU Exporter - GPU monitoring
  nvidia-exporter:
    image: mindprince/nvidia_gpu_prometheus_exporter:0.1
    container_name: erni-ki-nvidia-exporter
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    ports:
      - "127.0.0.1:9445:9445"
    restart: unless-stopped
    mem_limit: 256m
    mem_reservation: 128m
    cpus: "0.2"
    logging: *monitoring-logging
    security_opt:
      - no-new-privileges:true
    healthcheck:
      test: ["CMD-SHELL", "bash -c ': < /dev/tcp/localhost/9445'"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s
    # Health check disabled: minimal image lacks wget/curl/nc
    # Service monitored via Prometheus scrape success
    labels:
      # Allow auto-updates for NVIDIA Exporter
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=monitoring-stack"
    networks:
      - monitoring

  blackbox-exporter:
    image: prom/blackbox-exporter:v0.27.0 # updated from v0.25.0
    container_name: erni-ki-blackbox-exporter
    volumes:
      - ./conf/blackbox-exporter/blackbox.yml:/etc/blackbox_exporter/config.yml:ro
    ports:
      - "127.0.0.1:9115:9115"
    restart: unless-stopped
    mem_limit: 256m
    mem_reservation: 128m
    cpus: "0.2"
    logging: *monitoring-logging
    security_opt:
      - no-new-privileges:true
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "wget --no-verbose --tries=1 --spider http://localhost:9115/-/healthy || exit 1",
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    networks:
      - monitoring
      - frontend
    labels:
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=monitoring-stack"

  redis-exporter:
    depends_on:
      redis:
        condition: service_healthy
    image: oliver006/redis_exporter:v1.80.1
    container_name: erni-ki-redis-exporter
    user: "0:0"
    entrypoint: ["/opt/erni/bin/busybox", "sh", "/opt/erni/bin/redis-exporter-with-secret.sh"]
    environment:
      - REDIS_EXPORTER_INCL_SYSTEM_METRICS=true
      - REDIS_EXPORTER_LOG_FORMAT=txt
      - REDIS_EXPORTER_DEBUG=true
      - REDIS_USER=default
      - REDIS_ADDR=redis://redis:6379
    ports:
      - "127.0.0.1:9121:9121"
    restart: unless-stopped
    mem_limit: 256m
    mem_reservation: 128m
    cpus: "0.2"
    logging: *monitoring-logging
    security_opt:
      - no-new-privileges:true
    healthcheck:
      test:
        [
          "CMD",
          "/opt/erni/bin/busybox",
          "wget",
          "-q",
          "--tries=1",
          "--spider",
          "http://localhost:9121/metrics",
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    labels:
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=monitoring-stack"
    networks:
      - monitoring
      - data
    secrets:
      - redis_password
    volumes:
      - ./scripts/entrypoints/redis-exporter-with-secret.sh:/opt/erni/bin/redis-exporter-with-secret.sh:ro
      - ./scripts/entrypoints/busybox:/opt/erni/bin/busybox:ro

  ollama-exporter:
    depends_on:
      ollama:
        condition: service_healthy
    build:
      context: ./ops/ollama-exporter
      dockerfile: Dockerfile
    container_name: erni-ki-ollama-exporter
    environment:
      - OLLAMA_URL=http://ollama:11434
      - EXPORTER_PORT=9778
      - LOG_LEVEL=info
    ports:
      - "127.0.0.1:9778:9778"
    restart: unless-stopped
    mem_limit: 512m
    mem_reservation: 256m
    cpus: "0.5"
    logging: *monitoring-logging
    healthcheck:
      test:
        [
          "CMD-SHELL",
          'python -c "import urllib.request; urllib.request.urlopen(''http://localhost:9778/metrics'', timeout=5).read()"',
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 15s
    labels:
      # Disable Watchtower for locally-built image (no registry)
      - "com.centurylinklabs.watchtower.enable=false"
      - "com.centurylinklabs.watchtower.scope=monitoring-stack"
    networks:
      - monitoring
      - backend

  nginx-exporter:
    depends_on:
      nginx:
        condition: service_healthy
    image: nginx/nginx-prometheus-exporter:1.5.1
    container_name: erni-ki-nginx-exporter
    command:
      - "--nginx.scrape-uri=http://nginx:80/nginx_status"
      - "--web.listen-address=:9113"
    ports:
      - "127.0.0.1:9113:9113"
    restart: unless-stopped
    mem_limit: 256m
    mem_reservation: 128m
    cpus: "0.2"
    logging: *monitoring-logging
    security_opt:
      - no-new-privileges:true
    healthcheck:
      test: ["CMD", "/usr/bin/nginx-prometheus-exporter", "--version"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    labels:
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=monitoring-stack"
    networks:
      - monitoring
      - frontend
      - backend

  cadvisor:
    image: gcr.io/cadvisor/cadvisor:v0.52.1
    container_name: erni-ki-cadvisor
    command:
      - "--housekeeping_interval=10s"
      - "--max_housekeeping_interval=15s"
      - "--docker_only=true"
      - "--disable_metrics=disk,network,tcp,udp,percpu,sched,process"
      - "--store_container_labels=false"
      - "--whitelisted_container_labels=io.kubernetes.container.name,io.kubernetes.pod.name"
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
    ports:
      - "127.0.0.1:8081:8080"
    restart: unless-stopped
    mem_limit: 512m
    mem_reservation: 256m
    cpus: "0.5"
    logging: *monitoring-logging
    security_opt:
      - no-new-privileges:true
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "wget --no-verbose --tries=1 --spider http://localhost:8080/healthz || exit 1",
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    labels:
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=monitoring-stack"
    networks:
      - monitoring

  # === CENTRALIZED LOGGING ===

  fluent-bit:
    depends_on:
      loki:
        condition: service_healthy
      # Additional delay for Loki stabilization
      prometheus:
        condition: service_healthy
    environment:
      # Log level
      - FLB_LOG_LEVEL=info
      # HTTP server for metrics and health checks
      - FLB_HTTP_SERVER=On
      - FLB_HTTP_LISTEN=0.0.0.0
      - FLB_HTTP_PORT=2020
      # Health check endpoint
      - FLB_HEALTH_CHECK=On
    # Health check for Fluent Bit â†’ Loki integration monitoring
    # Health check disabled: minimal image lacks wget/curl/nc
    # Service monitored via Prometheus scrape success and Loki logs
    image: fluent/fluent-bit:4.2.0 # major update 3.x -> 4.2.0
    container_name: erni-ki-fluent-bit
    logging: *monitoring-logging
    restart: unless-stopped
    mem_limit: 512m
    mem_reservation: 256m
    cpus: "0.5"
    volumes:
      # Updated Fluent Bit configuration for Loki integration
      - ./conf/fluent-bit/fluent-bit.conf:/fluent-bit/etc/fluent-bit.conf:ro
      - ./conf/fluent-bit/parsers.conf:/fluent-bit/etc/parsers.conf:ro
      - ./conf/fluent-bit/scripts:/fluent-bit/scripts:ro
      - ./conf/fluent-bit/filters-log-optimization.conf:/fluent-bit/etc/filters-log-optimization.conf:ro
      - ./conf/fluent-bit/filters-optimized.conf:/fluent-bit/etc/filters-optimized.conf:ro
      - ./conf/fluent-bit/certs:/fluent-bit/etc/certs:ro
      # Dedicated volume for logs with SSD optimization (Phase 3)
      - erni-ki-logs:/var/log
      - erni-ki-fluent-db:/fluent-bit/db
      # Access to Docker socket for container log collection
      - /var/run/docker.sock:/var/run/docker.sock:ro
      # Access to Docker container json-files (tail input)
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
    ports:
      - "127.0.0.1:2020:2020" # HTTP API and health checks
      - "127.0.0.1:2021:2021" # Prometheus metrics (Phase 2)
      - "127.0.0.1:24224:24224" # Fluentd forward protocol
    healthcheck:
      test: ["CMD", "/fluent-bit/bin/fluent-bit", "--version"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    labels:
      # Allow auto-updates for Fluent Bit
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=logging-stack"
    networks:
      monitoring:
        aliases:
          - fluent-bit

  promtail:
    image: grafana/promtail:3.0.0
    container_name: erni-ki-promtail
    command: ["-config.file=/etc/promtail/config.yaml"]
    volumes:
      - ./conf/promtail/promtail-config.yaml:/etc/promtail/config.yaml:ro
      - ./data/promtail:/var/log/promtail
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
    networks:
      monitoring:
        aliases:
          - promtail
    logging: *monitoring-logging
    restart: unless-stopped
    mem_limit: 256m
    mem_reservation: 128m
    cpus: "0.3"
    healthcheck:
      test: ["CMD", "/usr/bin/promtail", "-version"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s
    labels:
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=logging-stack"

  # RAG Exporter - RAG SLA metrics (latency, sources)
  rag-exporter:
    build:
      context: ./conf
      dockerfile: Dockerfile.rag-exporter
    container_name: erni-ki-rag-exporter
    environment:
      - PORT=9808
      # Test request URL (default: OpenWebUI health);
      # can be replaced with real RAG endpoint
      - RAG_TEST_URL=http://openwebui:8080/health
      - RAG_TEST_INTERVAL=30
    ports:
      - "127.0.0.1:9808:9808"
    restart: unless-stopped
    mem_limit: 512m
    mem_reservation: 256m
    cpus: "0.5"
    logging: *monitoring-logging
    healthcheck:
      test:
        [
          "CMD-SHELL",
          'python -c ''import urllib.request,sys; urllib.request.urlopen("http://localhost:9808/metrics", timeout=5); print("ok")'' || exit 1',
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 15s
    labels:
      # Disable Watchtower for locally-built image (no registry)
      - "com.centurylinklabs.watchtower.enable=false"
      - "com.centurylinklabs.watchtower.scope=monitoring-stack"
    networks:
      - monitoring
      - backend

  # Webhook Receiver - Alertmanager alert processing
  webhook-receiver:
    depends_on:
      alertmanager:
        condition: service_healthy
    build:
      context: ./conf/webhook-receiver
      dockerfile: Dockerfile
    container_name: erni-ki-webhook-receiver
    environment:
      # Port for webhook endpoints
      - WEBHOOK_PORT=9093
      # Python settings
      - PYTHONUNBUFFERED=1
      - FLASK_ENV=production
      # Logging settings
      - LOG_LEVEL=INFO
    volumes:
      # Webhook receiver logs
      - ./data/webhook-logs:/app/logs
      # Alert processing scripts
      - ./conf/webhook-receiver/scripts:/app/scripts:ro
    ports:
      - "127.0.0.1:9095:9093" # Webhook endpoint (avoid conflict with Alertmanager 9093)
    restart: unless-stopped
    mem_limit: 256m
    mem_reservation: 128m
    cpus: "0.25"
    oom_score_adj: 250
    logging: *monitoring-logging
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9093/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    labels:
      # Disable Watchtower for locally-built image (no registry)
      - "com.centurylinklabs.watchtower.enable=false"
      - "com.centurylinklabs.watchtower.scope=monitoring-stack"
    networks:
      - backend
      - monitoring
# Using standard Docker bridge network (docker0)

# ============================================================================
# DEDICATED VOLUMES - Optimized volumes for performance (Phase 3)
# ============================================================================

volumes:
  # Dedicated volume for logs with SSD optimization
  erni-ki-logs:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${PWD}/data/logs-optimized
    labels:
      - "com.erni-ki.volume.type=logs"
      - "com.erni-ki.volume.optimization=ssd"
      - "com.erni-ki.volume.phase=3"

  # Dedicated volume for Fluent Bit database with high performance
  erni-ki-fluent-db:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${PWD}/data/fluent-bit-optimized
    labels:
      - "com.erni-ki.volume.type=database"
      - "com.erni-ki.volume.optimization=high-performance"
      - "com.erni-ki.volume.phase=3"

# ============================================================================
# DOCKER SECRETS - Secure storage for passwords and API keys
# ============================================================================
# Added: 2025-10-30 (Phase 1: Critical security fixes)
# All sensitive data moved from env files to Docker Secrets
secrets:
  # PostgreSQL password (used by: db, openwebui, mcposerver)
  postgres_password:
    file: ./secrets/postgres_password.txt

  # LiteLLM database password
  litellm_db_password:
    file: ./secrets/litellm_db_password.txt

  # LiteLLM API key (used by: openwebui, litellm)
  litellm_api_key:
    file: ./secrets/litellm_api_key.txt

  # Context7 API key (used by: mcposerver)
  context7_api_key:
    file: ./secrets/context7_api_key.txt

  # Watchtower HTTP API token
  watchtower_api_token:
    file: ./secrets/watchtower_api_token.txt
  # Restic repository password (Backrest)
  restic_password:
    file: ./secrets/restic_password.txt

  # Grafana admin password
  grafana_admin_password:
    file: ./secrets/grafana_admin_password.txt

  # DSN for Postgres exporter
  postgres_exporter_dsn:
    file: ./secrets/postgres_exporter_dsn.txt

  # Redis URL for redis-exporter
  redis_exporter_url:
    file: ./secrets/redis_exporter_url.txt

  # Cloudflare tunnel token
  cloudflared_tunnel_token:
    file: ./secrets/cloudflared_tunnel_token.txt

  # Redis password (used by Redis requirepass and clients)
  redis_password:
    file: ./secrets/redis_password.txt

  # OpenWebUI secret key (FastAPI SECRET_KEY)
  openwebui_secret_key:
    file: ./secrets/openwebui_secret_key.txt

  # LiteLLM master key for admin access
  litellm_master_key:
    file: ./secrets/litellm_master_key.txt

  # LiteLLM salt key for API key encryption
  litellm_salt_key:
    file: ./secrets/litellm_salt_key.txt

  # LiteLLM UI password
  litellm_ui_password:
    file: ./secrets/litellm_ui_password.txt
  openai_api_key:
    file: ./secrets/openai_api_key.txt
  searxng_secret_key:
    file: ./secrets/searxng_secret_key.txt
  slack_alert_webhook:
    file: ./secrets/slack_alert_webhook.txt
  pagerduty_routing_key:
    file: ./secrets/pagerduty_routing_key.txt
  # PublicAI API key for external models
  publicai_api_key:
    file: ./secrets/publicai_api_key.txt

  # Ragflow API key (MCP)
  ragflow_api_key:
    file: ./secrets/ragflow_api_key.txt

  # EdgeTTS API key (optional)
  edgetts_api_key:
    file: ./secrets/edgetts_api_key.txt

  # Docling picture description API config (optional)
  docling_picture_api:
    file: ./secrets/docling_picture_api.txt

# ============================================================================
# NETWORKS - INFRA-3a Segmentation
# ============================================================================
networks:
  # Public-facing ingress (Nginx, Cloudflared)
  frontend:
    driver: bridge

  # Application logic (OpenWebUI, APIs)
  # Needs outbound internet access for AI APIs
  backend:
    driver: bridge

  # Stateful services (DB, Redis) - Strictly internal
  data:
    driver: bridge
    internal: true

  # Observability stack
  monitoring:
    driver: bridge
