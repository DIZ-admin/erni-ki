# ============================================================================
# ERNI-KI 4-TIER LOGGING STRATEGY
# ============================================================================

# TIER 1: Critical Services (OpenWebUI, Ollama, PostgreSQL, Nginx)
# Dual logging: fluentd + json-file backup, maximum reliability
x-critical-logging: &critical-logging
  driver: "json-file"
  options:
    max-size: "10m"
    max-file: "5"
    labels: "service,version,environment,level=critical"
    tag: "critical.{{.Name}}"

# TIER 2: Important Services (SearXNG, Redis, Backrest, Auth, Cloudflared)
# Fluentd with buffering, standard reliability
x-important-logging: &important-logging
  driver: "json-file"
  options:
    max-size: "10m"
    max-file: "5"
    labels: "service,version,environment,level=important"

# TIER 3: Auxiliary Services (Docling, EdgeTTS, Tika, MCP)
# Fluentd with separate tags + tail fallback
x-auxiliary-logging: &auxiliary-logging
  driver: "json-file"
  options:
    max-size: "10m"
    max-file: "5"
    labels: "service,version,level=auxiliary"

# TIER 4: Monitoring Services (Prometheus, Grafana, Exporters)
# Minimal Fluentd logging with aggressive filtering
x-monitoring-logging: &monitoring-logging
  driver: "json-file"
  options:
    max-size: "10m"
    max-file: "5"
    labels: "service,version,level=monitoring"

# Legacy logging configuration - backward compatibility only
# Migration: new services must use tier-based logging critical/important/auxiliary/monitoring
# Deprecation: Q2 2025 (after all legacy services migrated)
x-logging: &default-logging
  driver: "json-file"
  options:
    max-size: "10m"
    max-file: "5"
    labels: "service,version"

services:
  # Infrastructure services
  watchtower:
    # Startup command with optimized parameters (schedule via env)
    command: --cleanup --label-enable --http-api-update --http-api-metrics
    env_file: env/watchtower.env
    environment:
      WATCHTOWER_HTTP_API_TOKEN: /run/secrets/watchtower_api_token
      DOCKER_API_VERSION: "1.44"
    healthcheck:
      test: ["CMD", "/watchtower", "--health-check"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s
    image: containrrr/watchtower:1.7.1 # Pinned version for production (latest stable)
    logging: *default-logging
    restart: unless-stopped
    mem_limit: 256m
    mem_reservation: 128m
    cpus: "0.2"
    oom_score_adj: 500
    # Run as root to guarantee access to docker.sock on hosts where docker group GID differs
    user: "0:0"
    group_add:
      - "125"
    # Port for HTTP API
    ports:
      - "127.0.0.1:8091:8080" # Localhost-only access for HTTP API
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    secrets:
      - watchtower_api_token
    labels:
      # Exclude Watchtower from self-monitoring
      - "com.centurylinklabs.watchtower.enable=false"
      # Label for log identification
      - "com.centurylinklabs.watchtower.scope=infrastructure"

  # PostgreSQL database with vector extension (optimized network configuration)
  db:
    env_file: env/db.env
    # Added: 2025-10-30 - Docker Secrets for secure password storage
    secrets:
      - postgres_password
    environment:
      # Override POSTGRES_PASSWORD from env file to use secret
      POSTGRES_PASSWORD_FILE: /run/secrets/postgres_password
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -d $${POSTGRES_DB} -U $${POSTGRES_USER}"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 20s
    image: pgvector/pgvector:pg17
    logging: *critical-logging
    restart: unless-stopped
    volumes:
      - ./data/postgres:/var/lib/postgresql/data
      # Custom PostgreSQL config with pg_stat_statements (added: 2025-11-04)
      - ./conf/postgres-enhanced/postgresql.conf:/etc/postgresql/postgresql.conf:ro
    # Using custom configuration
    command: postgres -c config_file=/etc/postgresql/postgresql.conf
    # Using standard Docker bridge network
    labels:
      # CRITICAL: Disable database auto-updates
      - "com.centurylinklabs.watchtower.enable=false"
      - "com.centurylinklabs.watchtower.monitor-only=true"
      - "com.centurylinklabs.watchtower.scope=critical-database"
    mem_limit: 4g
    cpus: "2.0"

  # Redis cache and queues (optimized network configuration)
  redis:
    env_file: env/redis.env
    # Using config file instead of CLI parameters
    # Updated: 2025-10-02 for support of active defragmentation
    command: [
        "/bin/sh",
        "-c",
        'set -euo pipefail; \\
        SECRET="$(cat /run/secrets/redis_password)"; \\
        CONF_TPL=/usr/local/etc/redis/redis.conf; \\
        CONF_OUT=/tmp/redis.conf; \\
        ACL_TPL=/usr/local/etc/redis/users.acl.template; \\
        ACL_OUT=/tmp/redis-users.acl; \\
        sed "s|__LOAD_FROM_DOCKER_SECRET__|${SECRET}|g" "$CONF_TPL" > "$CONF_OUT"; \\
        if [ -f "$ACL_TPL" ]; then \\
        sed "s|__LOAD_FROM_DOCKER_SECRET__|${SECRET}|g" "$ACL_TPL" > "$ACL_OUT"; \\
        chmod 600 "$ACL_OUT"; \\
        exec redis-server "$CONF_OUT" --aclfile "$ACL_OUT"; \\
        else \\
        exec redis-server "$CONF_OUT"; \\
        fi',
      ]
    healthcheck:
      test: ["CMD-SHELL", 'redis-cli -a "$(cat /run/secrets/redis_password)" ping | grep PONG']
      interval: 30s
      timeout: 3s
      retries: 5
      start_period: 20s
    image: redis:7.0.15-alpine # Pinned: avoid floating latest; 7.2 RDB format incompatible
    logging: *important-logging
    restart: unless-stopped
    volumes:
      - ./data/redis:/data
      - ./conf/redis/redis.conf:/usr/local/etc/redis/redis.conf:ro
      - ./conf/redis/users.acl:/usr/local/etc/redis/users.acl.template:ro
    secrets:
      - redis_password
    # Using standard Docker bridge network
    labels:
      # Allow auto-updates for Redis
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=cache-services"
    mem_limit: 1g
    cpus: "1.0"

  # LiteLLM Context Engineering Gateway (optimized network configuration)
  litellm:
    # Using latest LiteLLM version with thinking tokens support
    # Updated: 2025-11-18 to v1.80.0.rc.1 (routing fixes + security patches)
    image: ghcr.io/berriai/litellm:v1.80.0-stable.1
    container_name: erni-ki-litellm

    # Dependencies - start after critical services
    depends_on:
      db:
        condition: service_healthy
      ollama:
        condition: service_healthy

    # Environment file with configuration
    env_file: env/litellm.env
    entrypoint:
      - /bin/bash
      - /opt/erni/bin/litellm-entrypoint.sh

    # API port (internal Docker network)
    ports:
      - "127.0.0.1:4000:4000" # LiteLLM Proxy API (localhost only)
    expose:
      - "9109" # Prometheus metrics for PublicAI provider

    # Production restart policy
    restart: unless-stopped

    # Logging configuration
    logging: *auxiliary-logging

    # Health check for status monitoring (HTTP endpoint check)
    # Using Python urllib instead of curl (not installed in container) LiteLLM
    healthcheck:
      test:
        [
          "CMD-SHELL",
          'python3 -c "import urllib.request; urllib.request.urlopen(''http://localhost:4000/health/liveliness'', timeout=5).read()" || exit 1',
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s # Initialization time

    # Volume mounts for configuration
    volumes:
      - ./conf/litellm/config.yaml:/app/config.yaml:ro # Model configuration
      - ./data/litellm:/app/data # Logs and temporary files
      - ./scripts/entrypoints/litellm.sh:/opt/erni/bin/litellm-entrypoint.sh:ro # Secrets wrapper
      - ./conf/litellm/custom_providers:/app/custom_providers:ro # Custom providers LiteLLM

    # Command with basic settings
    command: ["--config", "/app/config.yaml", "--port", "4000", "--host", "0.0.0.0"]

    # Extra hosts for Ollama host connection
    extra_hosts:
      - "host.docker.internal:host-gateway"

    # Resource limits for stability (critical memory increase)
    mem_limit: 12g # Memory limit (increased to prevent OOM; was 8G)
    mem_reservation: 6g # Memory reservation (increased from 4G to 6G for buffer)
    cpus: "1.0" # CPU limit
    oom_score_adj: -300 # Protect proxy from aggressive OOM-killer

    # Using standard Docker bridge network

    # Monitoring labels
    labels:
      - "com.erni-ki.service=litellm"
      - "com.erni-ki.version=main-stable"
      - "com.erni-ki.description=Context Engineering Gateway"
      - "com.centurylinklabs.watchtower.enable=false"
      - "com.centurylinklabs.watchtower.monitor-only=true"
      - "com.centurylinklabs.watchtower.scope=ai-services"
    secrets:
      - litellm_db_password
      - litellm_api_key
      - litellm_master_key
      - litellm_salt_key
      - litellm_ui_password
      - openai_api_key
      - publicai_api_key
      - redis_password

  # JWT authentication service
  auth:
    build:
      context: ./auth
      dockerfile: Dockerfile
    env_file: env/auth.env
    logging: *important-logging
    ports:
      - "127.0.0.1:9092:9090" # Port changed to avoid conflict, localhost-only
    restart: unless-stopped
    mem_limit: 512m
    mem_reservation: 256m
    cpus: "0.5"
    healthcheck:
      test: ["CMD", "/app/main", "--health-check"]
      interval: 30s
      timeout: 3s
      retries: 3
      start_period: 5s
    labels:
      # Allow auto-updates for Auth service
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=auth-services"

  # Cloudflare tunnel for external access
  cloudflared:
    command: tunnel --no-autoupdate run
    depends_on:
      nginx:
        condition: service_healthy
      openwebui:
        condition: service_healthy
    env_file: env/cloudflared.env
    environment:
      # Explicit origin cert path (place cert.pem alongside config.yml/credentials-file)
      TUNNEL_ORIGIN_CERT: /home/nonroot/.cloudflared/cert.pem
    healthcheck:
      test: ["CMD", "cloudflared", "version"]
      interval: 60s
      timeout: 15s
      retries: 3
      start_period: 30s
    image: cloudflare/cloudflared:2025.11.1 # Updated: 2025-11-27 to 2025.11.1
    logging: *important-logging
    restart: unless-stopped
    mem_limit: 256m
    mem_reservation: 128m
    cpus: "0.25"
    volumes:
      - ./conf/cloudflare/config:/home/nonroot/.cloudflared
    # Using standard Docker bridge network
    labels:
      # Allow auto-updates for Cloudflared
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=tunnel-services"

  # Document extraction service
  # Text-to-speech service
  edgetts:
    env_file: env/edgetts.env
    healthcheck:
      test:
        [
          "CMD-SHELL",
          'python3 -c "import socket; s=socket.socket(); s.settimeout(5); s.connect((\"localhost\", 5050)); s.close()" || exit 1',
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    image: travisvn/openai-edge-tts@sha256:4e7e2773350a3296f301b5f66e361daad243bdc4b799eec32613fddcee849040 # updated: digest compatible with schema2
    logging: *auxiliary-logging
    ports:
      - "127.0.0.1:5050:5050"
    restart: unless-stopped
    mem_limit: 1g
    mem_reservation: 512m
    cpus: "0.5"
    labels:
      # Allow auto-updates for EdgeTTS
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=text-to-speech"

  # Apache Tika file processing service
  tika:
    env_file: env/tika.env
    healthcheck:
      test: ["CMD-SHELL", "timeout 5 bash -c '</dev/tcp/localhost/9998' || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    image: apache/tika:3.2.3.0-full # updated tag (was digest)
    logging: *auxiliary-logging
    ports:
      - "127.0.0.1:9998:9998"
    restart: unless-stopped
    mem_limit: 1g
    mem_reservation: 512m
    cpus: "0.5"
    labels:
      # Allow auto-updates for Tika
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=document-processing"

  # MCPO server for request processing
  mcposerver:
    command: ["--config", "/app/conf/config.json"]
    depends_on:
      db:
        condition: service_healthy
    env_file: env/mcposerver.env
    healthcheck:
      test:
        [
          "CMD-SHELL",
          'python3 -c "import urllib.request; urllib.request.urlopen(''http://localhost:8000/docs'', timeout=5).read()" || exit 1',
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    image: ghcr.io/open-webui/mcpo:git-91e8f94 # v0.0.19 tag not available in registry; keep stable commit
    logging: *auxiliary-logging
    ports:
      - "127.0.0.1:8000:8000"
    restart: unless-stopped
    mem_limit: 512m
    mem_reservation: 256m
    cpus: "0.5"
    volumes:
      - ./conf/mcposerver:/app/conf:ro
      - ./data:/app/data
    # Using standard Docker bridge network
    labels:
      # Allow auto-updates for MCP Server
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=ai-services"

  # SearXNG search engine (optimized network configuration)
  searxng:
    depends_on:
      redis:
        condition: service_healthy
    env_file: env/searxng.env
    healthcheck:
      test:
        [
          "CMD-SHELL",
          'wget -q --spider --header="User-Agent: OpenWebUI-HealthCheck/1.0" --header="X-Real-IP: 127.0.0.1" --header="X-Forwarded-For: 127.0.0.1" --header="Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8" --header="Accept-Language: en-US,en;q=0.5" http://localhost:8080/ || exit 1',
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    image: searxng/searxng:latest # refreshed tag (digest 2025-11-28)
    logging: *important-logging
    restart: unless-stopped
    volumes:
      - ./conf/searxng/settings.yml:/etc/searxng/settings.yml:ro
      - ./conf/searxng/uwsgi.ini:/etc/searxng/uwsgi.ini:ro
      - ./conf/searxng/limiter.toml:/etc/searxng/limiter.toml:ro
      - ./conf/searxng/favicons.toml:/etc/searxng/favicons.toml:ro
      - ./scripts/entrypoints/searxng.sh:/opt/erni/bin/searxng-entrypoint.sh:ro
    secrets:
      - redis_password
    # Using standard Docker bridge network
    labels:
      # Allow auto-updates for SearXNG
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=search-services"
    mem_limit: 1g
    cpus: "1.0"

  # Ollama LLM server with GPU acceleration (optimized network configuration)
  ollama:
    runtime: nvidia
    mem_limit: 24g
    mem_reservation: 12g
    cpus: "12.0"
    oom_score_adj: -900
    environment:
      - NVIDIA_VISIBLE_DEVICES=${OLLAMA_GPU_VISIBLE_DEVICES:-0}
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - CUDA_VISIBLE_DEVICES=${OLLAMA_CUDA_VISIBLE_DEVICES:-0}
      - CUDA_DEVICE_ORDER=PCI_BUS_ID
    env_file: env/ollama.env
    # GPU memory limits optimized for RTX 5000 (16GB VRAM)
    # Ollama gets GPU priority (VRAM management via OLLAMA_MAX_VRAM=0.75)
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 30s
      timeout: 3s
      retries: 5
      start_period: 10s
    image: ollama/ollama:0.13.0 # Updated to 0.13.0 (GPU/Vulkan improvements)
    logging: *critical-logging
    ports:
      - "127.0.0.1:11434:11434"
    restart: unless-stopped
    volumes:
      - ./data/ollama:/root/.ollama
    # Using standard Docker bridge network
    labels:
      # CRITICAL: Disable Ollama auto-updates (GPU)
      - "com.centurylinklabs.watchtower.enable=false"
      - "com.centurylinklabs.watchtower.monitor-only=true"
      - "com.centurylinklabs.watchtower.scope=critical-ai-gpu"

  # Nginx reverse proxy (optimized network configuration)
  nginx:
    depends_on:
      auth:
        condition: service_healthy
      openwebui:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "/etc/nginx/healthcheck.sh"]
      interval: 60s
      timeout: 30s
      retries: 3
      start_period: 30s
    # Updated: 2025-11-04 from 1.28.0 to 1.29.3 (mainline with performance improvements)
    image: nginx:1.29.3
    logging: *critical-logging
    ports:
      - "80:80"
      - "443:443"
      - "8080:8080"
    restart: unless-stopped
    volumes:
      - ./conf/nginx/conf.d/default.conf:/etc/nginx/conf.d/default.conf
      - ./conf/nginx/nginx.conf:/etc/nginx/nginx.conf
      - ./conf/nginx/ssl:/etc/nginx/ssl
      - ./conf/nginx/includes:/etc/nginx/includes
      - ./conf/nginx/healthcheck.sh:/etc/nginx/healthcheck.sh:ro
      - ./data/nginx/webroot:/var/www/html
      - ./data/nginx/logs:/var/log/nginx
    # Using standard Docker bridge network
    labels:
      # CRITICAL: Disable Nginx auto-updates (critical proxy server)
      - "com.centurylinklabs.watchtower.enable=false"
      - "com.centurylinklabs.watchtower.monitor-only=true"
      - "com.centurylinklabs.watchtower.scope=critical-proxy"
    mem_limit: 512m
    cpus: "1.0"

  # OpenWebUI main interface with GPU acceleration (optimized network configuration)
  openwebui:
    depends_on:
      auth:
        condition: service_healthy
      db:
        condition: service_healthy
      litellm:
        condition: service_healthy
      ollama:
        condition: service_healthy
      redis:
        condition: service_healthy
    image: ghcr.io/open-webui/open-webui:v0.6.40 # Updated: 2025-11-28 to v0.6.40 (latest stable patch)
    runtime: nvidia
    mem_limit: 8g
    mem_reservation: 4g
    cpus: "4.0"
    oom_score_adj: -600
    environment:
      - NVIDIA_VISIBLE_DEVICES=${OPENWEBUI_GPU_VISIBLE_DEVICES:-0}
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - CUDA_VISIBLE_DEVICES=${OPENWEBUI_CUDA_VISIBLE_DEVICES:-0}
    env_file: env/openwebui.env
    entrypoint:
      - /bin/bash
      - /opt/erni/bin/openwebui-entrypoint.sh
    healthcheck:
      test: ["CMD-SHELL", "curl --fail http://localhost:8080/health || exit 1"]
      interval: 30s
      timeout: 3s
      retries: 5
      start_period: 10s
    logging: *critical-logging
    restart: unless-stopped
    volumes:
      - ./data/openwebui:/app/backend/data
      - ./data/docling/shared:/app/backend/data/docling-shared
      - ./scripts/entrypoints/openwebui.sh:/opt/erni/bin/openwebui-entrypoint.sh:ro
      # Shared volume for file transfer to Docling (default mount for exchange)
      # Entry-point wrapper injects secrets from Docker secrets
    secrets:
      - postgres_password
      - litellm_api_key
      - openwebui_secret_key
      - redis_password
    # Using standard Docker bridge network
    labels:
      # Allow auto-updates for OpenWebUI
      - "com.centurylinklabs.watchtower.enable=false"
      - "com.centurylinklabs.watchtower.monitor-only=true"
      - "com.centurylinklabs.watchtower.scope=web-interface"

  # Docling OCR/ingestion pipeline (GPU aware)
  docling:
    # Official Docling Serve GPU image (CUDA 12.6)
    # Override DOCLING_IMAGE in .env with pinned digest/tag
    image: ${DOCLING_IMAGE:-ghcr.io/docling-project/docling-serve-cu126:main}
    depends_on:
      ollama:
        condition: service_healthy
      redis:
        condition: service_healthy
    runtime: nvidia
    mem_limit: 12g
    mem_reservation: 8g
    cpus: "8.0"
    oom_score_adj: -500
    env_file: env/docling.env
    environment:
      - NVIDIA_VISIBLE_DEVICES=${DOCLING_GPU_VISIBLE_DEVICES:-0}
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - CUDA_VISIBLE_DEVICES=${DOCLING_CUDA_VISIBLE_DEVICES:-0}
      - CUDA_DEVICE_ORDER=PCI_BUS_ID
    healthcheck:
      test: ["CMD-SHELL", "curl --fail http://localhost:5001/health || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s
    logging: *auxiliary-logging
    restart: unless-stopped
    volumes:
      - ./data/docling/models:/opt/app-root/src/.cache/huggingface
      - ./data/docling/easyocr:/opt/app-root/src/.EasyOCR
      - ./data/docling/docling-models:/opt/app-root/src/.cache/docling
      - ./data/docling/docling-models:/docling-artifacts
      - ./data/docling/shared:/docling-shared
    labels:
      - "com.centurylinklabs.watchtower.enable=false"
      - "com.centurylinklabs.watchtower.scope=document-processing"
      - "com.erni-ki.service=docling"

  backrest:
    depends_on:
      - db
      - redis
    env_file: env/backrest.env
    healthcheck:
      test: ["CMD-SHELL", "curl -s http://localhost:9898/ >/dev/null || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s
    # Updated: 2025-10-02 from v1.4.0 to v1.10.1 (latest stable with fixes)
    image: garethgeorge/backrest:v1.10.1
    logging: *important-logging
    ports:
      - "127.0.0.1:9898:9898"
    restart: unless-stopped
    mem_limit: 512m
    mem_reservation: 256m
    cpus: "0.5"
    volumes:
      - ./data/backrest:/data
      - ./conf/backrest:/config
      - ./cache/backrest:/cache
      - ./tmp/backrest:/tmp
      - ./data:/backup-sources/data:ro
      - ./conf:/backup-sources/conf:ro
      - ./env:/backup-sources/env:ro
      - ./.config-backup:/backup-sources/.config-backup
      - /var/run/docker.sock:/var/run/docker.sock:ro
    labels:
      # Allow auto-updates for Backrest
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=backup-services"

  # ============================================================================
  # MONITORING AND LOGGING
  # ============================================================================

  # Prometheus - metrics collection
  prometheus:
    image: prom/prometheus:v3.7.3 # Updated from v3.0.0
    container_name: erni-ki-prometheus
    depends_on:
      postgres-exporter:
        condition: service_healthy
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
      - "--storage.tsdb.retention.time=30d"
      - "--storage.tsdb.retention.size=10GB"
      - "--web.console.libraries=/etc/prometheus/console_libraries"
      - "--web.console.templates=/etc/prometheus/consoles"
      - "--web.external-url=http://prometheus.erni-ki.local"
    volumes:
      - ./conf/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./conf/prometheus/alert_rules.yml:/etc/prometheus/alert_rules.yml:ro
      - ./conf/prometheus/alerts.yml:/etc/prometheus/alerts.yml:ro # ADDED 2025-10-24: System alerts
      - ./conf/prometheus/rules:/etc/prometheus/rules:ro
      - ./conf/prometheus/alerts:/etc/prometheus/alerts:ro # Added: 2025-10-02 for litellm-memory.yml
      - ./data/prometheus:/prometheus
    ports:
      - "127.0.0.1:9091:9090"
    restart: unless-stopped
    mem_limit: 2g
    mem_reservation: 1g
    cpus: "1.5"
    logging: *monitoring-logging
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "wget --no-verbose --tries=1 --spider http://localhost:9090/-/healthy || exit 1",
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    labels:
      # Allow auto-updates for Prometheus
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=monitoring-stack"
    # Using standard Docker bridge network

  # Grafana - dashboard and visualization
  grafana:
    depends_on:
      prometheus:
        condition: service_healthy
    # Updated: 2025-10-02 from 10.2.0 to 11.6.6 (latest stable 11.x version)
    image: grafana/grafana:12.3.0 # Updated from 11.6.8 (12.x evaluation)
    container_name: erni-ki-grafana
    user: "0"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD__FILE=/run/secrets/grafana_admin_password
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_SERVER_ROOT_URL=http://grafana.erni-ki.local
      - GF_SERVER_SERVE_FROM_SUB_PATH=true
      - GF_ANALYTICS_REPORTING_ENABLED=false
      - GF_ANALYTICS_CHECK_FOR_UPDATES=false
    volumes:
      - ./data/grafana:/var/lib/grafana
      # Updated paths for optimized configurations
      - ./conf/grafana/provisioning:/etc/grafana/provisioning:ro
      - ./conf/grafana/dashboards:/var/lib/grafana/dashboards:ro
    ports:
      - "127.0.0.1:3000:3000"
    restart: unless-stopped
    mem_limit: 1g
    mem_reservation: 512m
    cpus: "0.5"
    logging: *monitoring-logging
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:3000/api/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    labels:
      # Allow auto-updates for Grafana
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=monitoring-stack"
    secrets:
      - source: grafana_admin_password
        target: grafana_admin_password
        mode: 0444
    # Using standard Docker bridge network

  # Uptime Kuma - visual ERNI-KI status
  uptime-kuma:
    image: louislam/uptime-kuma:2.0.2
    container_name: erni-ki-uptime-kuma
    env_file: env/uptime-kuma.env
    restart: always
    logging: *monitoring-logging
    ports:
      - "127.0.0.1:3001:3001" # Localhost-only
    volumes:
      - ./data/uptime-kuma:/app/data
    healthcheck:
      test: ["CMD", "node", "/app/extra/healthcheck.js"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    mem_limit: 512m
    mem_reservation: 256m
    cpus: "0.5"
    labels:
      # Allow auto-updates for Uptime Kuma
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=monitoring-stack"
    networks:
      default:
        aliases:
          - uptime-kuma

  # Loki - modern logging system (Elasticsearch replacement)
  loki:
    image: grafana/loki:3.6.2 # Updated from 3.0.0
    container_name: erni-ki-loki
    logging: *monitoring-logging
    restart: unless-stopped
    ports:
      - "127.0.0.1:3100:3100"
    mem_limit: 2g
    mem_reservation: 1g
    cpus: "1.0"
    volumes:
      - ./conf/loki/loki-config.yaml:/etc/loki/local-config.yaml:ro
      - ./data/loki:/loki
      - ./conf/loki/tls:/etc/loki/tls:ro
    command:
      - -config.file=/etc/loki/local-config.yaml
      - -config.expand-env=true
    healthcheck:
      test: ["CMD", "/usr/bin/loki", "--version"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    labels:
      # Allow auto-updates for Loki
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=logging-stack"

  # Alertmanager - alert management
  alertmanager:
    image: prom/alertmanager:v0.29.0 # updated from v0.27.0
    container_name: erni-ki-alertmanager
    command:
      - "--config.file=/etc/alertmanager/alertmanager.yml"
      - "--storage.path=/alertmanager"
      - "--web.external-url=http://alertmanager.erni-ki.local"
      - "--cluster.listen-address=0.0.0.0:9094"
      # Cluster parameters to resolve queue overflow issue
      - "--cluster.gossip-interval=500ms" # Increased from default 200ms
      - "--cluster.pushpull-interval=120s" # Increased from default 60s
      - "--cluster.tcp-timeout=15s" # Increased for connection stability
      - "--cluster.probe-timeout=1s" # Optimized for fast diagnostics
      - "--cluster.probe-interval=2s" # Cluster node check interval
    volumes:
      - ./conf/alertmanager/alertmanager.yml:/etc/alertmanager/alertmanager.yml:ro
      - ./data/alertmanager:/alertmanager
    ports:
      - "127.0.0.1:9093:9093"
      - "127.0.0.1:9094:9094"
    restart: unless-stopped
    mem_limit: 512m
    mem_reservation: 256m
    cpus: "0.5"
    logging: *monitoring-logging
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "wget --no-verbose --tries=1 --spider http://localhost:9093/-/healthy || exit 1",
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    labels:
      # Allow auto-updates for Alertmanager
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=monitoring-stack"
    secrets:
      - source: slack_alert_webhook
        target: slack_alert_webhook
        mode: 0444
      - source: pagerduty_routing_key
        target: pagerduty_routing_key
        mode: 0444
    # Using standard Docker bridge network

  # Node Exporter - system metrics
  node-exporter:
    image: prom/node-exporter:v1.10.2 # updated from v1.8.2
    container_name: erni-ki-node-exporter
    command:
      - "--path.procfs=/host/proc"
      - "--path.rootfs=/rootfs"
      - "--path.sysfs=/host/sys"
      - "--path.udev.data=/host/run/udev/data" # Path to udev data for diskstats collector
      - "--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)"
      - "--no-collector.systemd" # Disable systemd collector (no dbus access in container)
      # FIXED 2025-10-24: error instead of warn to suppress broken pipe errors
      # Broken pipe errors are normal behavior when client closes connection
      - "--log.level=error"
      - "--collector.processes"
      - "--web.disable-exporter-metrics" # Disable exporter's own metrics to reduce load
      - "--collector.textfile.directory=/var/lib/node_exporter/textfile_collector"
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
      - /run/udev/data:/host/run/udev/data:ro # Mount udev data for diskstats
      - /run/systemd/private:/run/systemd/private:ro
      - ./data/node-exporter-textfile:/var/lib/node_exporter/textfile_collector:ro
    ports:
      - "127.0.0.1:9101:9100"
    pid: host
    restart: unless-stopped
    mem_limit: 256m
    mem_reservation: 128m
    cpus: "0.2"
    logging: *monitoring-logging
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "wget --no-verbose --tries=1 --spider http://localhost:9100/metrics || exit 1",
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    labels:
      # Allow auto-updates for Node Exporter
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=monitoring-stack"
    # Using standard Docker bridge network

  # Postgres Exporter - PostgreSQL monitoring
  postgres-exporter:
    depends_on:
      db:
        condition: service_healthy
    # Rollback to prometheuscommunity image 2025-11-07: wrouesnel lacks wget/curl for healthcheck
    image: prometheuscommunity/postgres-exporter:v0.18.1
    container_name: erni-ki-postgres-exporter
    user: "0"
    environment:
      - PG_EXPORTER_DISABLE_DEFAULT_METRICS=false
      - PG_EXPORTER_DISABLE_SETTINGS_METRICS=true
      - PG_EXPORTER_AUTO_DISCOVER_DATABASES=true
      - PG_EXPORTER_EXCLUDE_DATABASES=template0,template1,postgres
    volumes:
      - ./scripts/infrastructure/postgres-exporter-entrypoint.sh:/entrypoint/postgres-exporter.sh:ro
      - ./conf/postgres-exporter/config.yml:/etc/postgres_exporter.yml:ro
    # Publish port 9188 for socat proxy (2025-11-07)
    ports:
      - "127.0.0.1:9188:9188"
    restart: unless-stopped
    mem_limit: 256m
    mem_reservation: 128m
    cpus: "0.2"
    entrypoint:
      - /entrypoint/postgres-exporter.sh
    command:
      - --config.file=/etc/postgres_exporter.yml
      - --no-collector.stat_bgwriter
    logging: *monitoring-logging
    healthcheck:
      test: ["CMD-SHELL", "wget --quiet --tries=1 --spider http://localhost:9187/metrics || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    labels:
      # Allow auto-updates for Postgres Exporter
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=monitoring-stack"
    secrets:
      - source: postgres_exporter_dsn
        target: /etc/postgres_exporter_dsn.txt
        mode: 0444
    # Using standard Docker bridge network

  # Socat Proxy - IPv4 to IPv6 proxy for Postgres Exporter
  # Solution: Postgres Exporter listens only on IPv6, but Prometheus connects via IPv4
  # Using network_mode: service for localhost exporter access (2025-11-07)
  postgres-exporter-proxy:
    depends_on:
      postgres-exporter:
        condition: service_healthy
    image: alpine/socat@sha256:86b69d2e491f6c32a9ce5ec8e3489c195ba314a0f99ca30debf6007376e795e5
    container_name: erni-ki-postgres-exporter-proxy
    network_mode: "service:postgres-exporter"
    entrypoint: ["socat"]
    command: ["TCP4-LISTEN:9188,fork,reuseaddr", "TCP6:[::1]:9187"]
    restart: unless-stopped
    mem_limit: 256m
    mem_reservation: 128m
    cpus: "0.2"
    logging: *monitoring-logging
    labels:
      - "com.centurylinklabs.watchtower.enable=false"
      - "com.centurylinklabs.watchtower.scope=monitoring-stack"
    healthcheck:
      test: ["CMD-SHELL", "pgrep socat"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s

  # NVIDIA GPU Exporter - GPU monitoring
  nvidia-exporter:
    image: mindprince/nvidia_gpu_prometheus_exporter:0.1
    container_name: erni-ki-nvidia-exporter
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    ports:
      - "127.0.0.1:9445:9445"
    restart: unless-stopped
    mem_limit: 256m
    mem_reservation: 128m
    cpus: "0.2"
    logging: *monitoring-logging
    healthcheck:
      test: ["CMD-SHELL", "bash -c ': < /dev/tcp/localhost/9445'"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s
    # Health check disabled: minimal image lacks wget/curl/nc
    # Service monitored via Prometheus scrape success
    labels:
      # Allow auto-updates for NVIDIA Exporter
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=monitoring-stack"
    # Using standard Docker bridge network

  # Blackbox Exporter - availability monitoring
  blackbox-exporter:
    image: prom/blackbox-exporter:v0.27.0 # updated from v0.25.0
    container_name: erni-ki-blackbox-exporter
    volumes:
      - ./conf/blackbox-exporter/blackbox.yml:/etc/blackbox_exporter/config.yml:ro
    ports:
      - "127.0.0.1:9115:9115"
    restart: unless-stopped
    mem_limit: 256m
    mem_reservation: 128m
    cpus: "0.2"
    logging: *monitoring-logging
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "wget --no-verbose --tries=1 --spider http://localhost:9115/-/healthy || exit 1",
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    # Using standard Docker bridge network
    labels:
      # Allow auto-updates for Blackbox Exporter
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=monitoring-stack"

  # Redis Exporter - Redis monitoring
  redis-exporter:
    depends_on:
      redis:
        condition: service_healthy
    # Updated: 2025-10-02 from v1.55.0 to v1.62.0 (stable version without v1.77.0 issues)
    image: oliver006/redis_exporter:v1.80.1 # updated from v1.62.0
    container_name: erni-ki-redis-exporter
    # Fixed: 2025-11-07 using full URL with password (format redis://:password@host:port)
    environment:
      - REDIS_EXPORTER_INCL_SYSTEM_METRICS=true
      - REDIS_EXPORTER_LOG_FORMAT=txt
      - REDIS_EXPORTER_DEBUG=true
      - REDIS_ADDR=redis://:${REDIS_PASSWORD}@redis:6379
    ports:
      - "127.0.0.1:9121:9121"
    restart: unless-stopped
    mem_limit: 256m
    mem_reservation: 128m
    cpus: "0.2"
    logging: *monitoring-logging
    healthcheck:
      test: ["CMD", "/redis_exporter", "--version"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    # Health check disabled: minimal image lacks wget/curl/nc
    # Service monitored via Prometheus scrape success
    labels:
      # Allow auto-updates for Redis Exporter
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=monitoring-stack"
    # Using standard Docker bridge network

  # Ollama Exporter - AI services monitoring
  ollama-exporter:
    depends_on:
      ollama:
        condition: service_healthy
    build:
      context: ./ops/ollama-exporter
      dockerfile: Dockerfile
    container_name: erni-ki-ollama-exporter
    environment:
      - OLLAMA_URL=http://ollama:11434
      - EXPORTER_PORT=9778
      - LOG_LEVEL=info
    ports:
      - "127.0.0.1:9778:9778"
    restart: unless-stopped
    mem_limit: 512m
    mem_reservation: 256m
    cpus: "0.5"
    logging: *monitoring-logging
    healthcheck:
      test:
        [
          "CMD-SHELL",
          'python -c "import urllib.request; urllib.request.urlopen(''http://localhost:9778/metrics'', timeout=5).read()"',
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 15s
    labels:
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=monitoring-stack"

  # Nginx Exporter - web server monitoring
  nginx-exporter:
    depends_on:
      nginx:
        condition: service_healthy
    # Updated: 2025-10-02 from 1.1.0 to 1.4.2 (latest stable version)
    image: nginx/nginx-prometheus-exporter:1.5.1 # updated from 1.1.0
    container_name: erni-ki-nginx-exporter
    command:
      - "--nginx.scrape-uri=http://nginx:80/nginx_status" # Fixed port from 8080 to 80
      - "--web.listen-address=:9113"
    ports:
      - "127.0.0.1:9113:9113"
    restart: unless-stopped
    mem_limit: 256m
    mem_reservation: 128m
    cpus: "0.2"
    logging: *monitoring-logging
    healthcheck:
      test: ["CMD", "/usr/bin/nginx-prometheus-exporter", "--version"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    labels:
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=monitoring-stack"

  # cAdvisor - container monitoring
  cadvisor:
    # Updated: 2025-10-02 from v0.47.2 to v0.52.1 (latest available version)
    image: gcr.io/cadvisor/cadvisor:v0.52.1 # keep stable tag (v0.53.0 not available)
    container_name: erni-ki-cadvisor
    command:
      - "--housekeeping_interval=10s"
      - "--max_housekeeping_interval=15s"
      - "--docker_only=true"
      - "--disable_metrics=disk,network,tcp,udp,percpu,sched,process"
      - "--store_container_labels=false"
      - "--whitelisted_container_labels=io.kubernetes.container.name,io.kubernetes.pod.name"
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
    ports:
      - "127.0.0.1:8081:8080"
    restart: unless-stopped
    mem_limit: 512m
    mem_reservation: 256m
    cpus: "0.5"
    logging: *monitoring-logging
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "wget --no-verbose --tries=1 --spider http://localhost:8080/healthz || exit 1",
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    labels:
      # Allow auto-updates for cAdvisor
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=monitoring-stack"
    # Using standard Docker bridge network

  # === CENTRALIZED LOGGING ===

  # Fluent Bit - log collector and processor
  fluent-bit:
    depends_on:
      loki:
        condition: service_healthy
      # Additional delay for Loki stabilization
      prometheus:
        condition: service_healthy
    environment:
      # Log level
      - FLB_LOG_LEVEL=info
      # HTTP server for metrics and health checks
      - FLB_HTTP_SERVER=On
      - FLB_HTTP_LISTEN=0.0.0.0
      - FLB_HTTP_PORT=2020
      # Health check endpoint
      - FLB_HEALTH_CHECK=On
    # Health check for Fluent Bit â†’ Loki integration monitoring
    # Health check disabled: minimal image lacks wget/curl/nc
    # Service monitored via Prometheus scrape success and Loki logs
    image: fluent/fluent-bit:4.2.0 # major update 3.x -> 4.2.0
    container_name: erni-ki-fluent-bit
    logging: *monitoring-logging
    restart: unless-stopped
    mem_limit: 512m
    mem_reservation: 256m
    cpus: "0.5"
    volumes:
      # Updated Fluent Bit configuration for Loki integration
      - ./conf/fluent-bit/fluent-bit.conf:/fluent-bit/etc/fluent-bit.conf:ro
      - ./conf/fluent-bit/parsers.conf:/fluent-bit/etc/parsers.conf:ro
      - ./conf/fluent-bit/scripts:/fluent-bit/scripts:ro
      - ./conf/fluent-bit/filters-log-optimization.conf:/fluent-bit/etc/filters-log-optimization.conf:ro
      - ./conf/fluent-bit/filters-optimized.conf:/fluent-bit/etc/filters-optimized.conf:ro
      - ./conf/fluent-bit/certs:/fluent-bit/etc/certs:ro
      # Dedicated volume for logs with SSD optimization (Phase 3)
      - erni-ki-logs:/var/log
      - erni-ki-fluent-db:/fluent-bit/db
      # Access to Docker socket for container log collection
      - /var/run/docker.sock:/var/run/docker.sock:ro
      # Access to Docker container json-files (tail input)
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
    ports:
      - "127.0.0.1:2020:2020" # HTTP API and health checks
      - "127.0.0.1:2021:2021" # Prometheus metrics (Phase 2)
      - "127.0.0.1:24224:24224" # Fluentd forward protocol
    healthcheck:
      test: ["CMD", "/fluent-bit/bin/fluent-bit", "--version"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    labels:
      # Allow auto-updates for Fluent Bit
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=logging-stack"
    networks:
      default:
        aliases:
          - fluent-bit

  # RAG Exporter - RAG SLA metrics (latency, sources)
  rag-exporter:
    build:
      context: ./conf
      dockerfile: Dockerfile.rag-exporter
    container_name: erni-ki-rag-exporter
    environment:
      - PORT=9808
      # Test request URL (default: OpenWebUI health);
      # can be replaced with real RAG endpoint
      - RAG_TEST_URL=http://openwebui:8080/health
      - RAG_TEST_INTERVAL=30
    ports:
      - "127.0.0.1:9808:9808"
    restart: unless-stopped
    mem_limit: 512m
    mem_reservation: 256m
    cpus: "0.5"
    logging: *monitoring-logging
    healthcheck:
      test:
        [
          "CMD-SHELL",
          'python -c ''import urllib.request,sys; urllib.request.urlopen("http://localhost:9808/metrics", timeout=5); print("ok")'' || exit 1',
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 15s
    labels:
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=monitoring-stack"

  # Webhook Receiver - Alertmanager alert processing
  webhook-receiver:
    depends_on:
      alertmanager:
        condition: service_healthy
    build:
      context: ./conf/webhook-receiver
      dockerfile: Dockerfile
    container_name: erni-ki-webhook-receiver
    environment:
      # Port for webhook endpoints
      - WEBHOOK_PORT=9093
      # Python settings
      - PYTHONUNBUFFERED=1
      - FLASK_ENV=production
      # Logging settings
      - LOG_LEVEL=INFO
    volumes:
      # Webhook receiver logs
      - ./data/webhook-logs:/app/logs
      # Alert processing scripts
      - ./conf/webhook-receiver/scripts:/app/scripts:ro
    ports:
      - "127.0.0.1:9095:9093" # Webhook endpoint (avoid conflict with Alertmanager 9093)
    restart: unless-stopped
    mem_limit: 256m
    mem_reservation: 128m
    cpus: "0.25"
    oom_score_adj: 250
    logging: *monitoring-logging
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9093/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    labels:
      # Allow auto-updates for Webhook Receiver
      - "com.centurylinklabs.watchtower.enable=true"
      - "com.centurylinklabs.watchtower.scope=monitoring-stack"
# Using standard Docker bridge network (docker0)

# ============================================================================
# DEDICATED VOLUMES - Optimized volumes for performance (Phase 3)
# ============================================================================

volumes:
  # Dedicated volume for logs with SSD optimization
  erni-ki-logs:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${PWD}/data/logs-optimized
    labels:
      - "com.erni-ki.volume.type=logs"
      - "com.erni-ki.volume.optimization=ssd"
      - "com.erni-ki.volume.phase=3"

  # Dedicated volume for Fluent Bit database with high performance
  erni-ki-fluent-db:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${PWD}/data/fluent-bit-optimized
    labels:
      - "com.erni-ki.volume.type=database"
      - "com.erni-ki.volume.optimization=high-performance"
      - "com.erni-ki.volume.phase=3"

# ============================================================================
# DOCKER SECRETS - Secure storage for passwords and API keys
# ============================================================================
# Added: 2025-10-30 (Phase 1: Critical security fixes)
# All sensitive data moved from env files to Docker Secrets
secrets:
  # PostgreSQL password (used by: db, openwebui, mcposerver)
  postgres_password:
    file: ./secrets/postgres_password.txt

  # LiteLLM database password
  litellm_db_password:
    file: ./secrets/litellm_db_password.txt

  # LiteLLM API key (used by: openwebui, litellm)
  litellm_api_key:
    file: ./secrets/litellm_api_key.txt

  # Context7 API key (used by: mcposerver)
  context7_api_key:
    file: ./secrets/context7_api_key.txt

  # Watchtower HTTP API token
  watchtower_api_token:
    file: ./secrets/watchtower_api_token.txt

  # Grafana admin password
  grafana_admin_password:
    file: ./secrets/grafana_admin_password.txt

  # DSN for Postgres exporter
  postgres_exporter_dsn:
    file: ./secrets/postgres_exporter_dsn.txt

  # Redis URL for redis-exporter
  redis_exporter_url:
    file: ./secrets/redis_exporter_url.txt

  # Redis password (used by Redis requirepass and clients)
  redis_password:
    file: ./secrets/redis_password.txt

  # OpenWebUI secret key (FastAPI SECRET_KEY)
  openwebui_secret_key:
    file: ./secrets/openwebui_secret_key.txt

  # LiteLLM master key for admin access
  litellm_master_key:
    file: ./secrets/litellm_master_key.txt

  # LiteLLM salt key for API key encryption
  litellm_salt_key:
    file: ./secrets/litellm_salt_key.txt

  # LiteLLM UI password
  litellm_ui_password:
    file: ./secrets/litellm_ui_password.txt
  openai_api_key:
    file: ./secrets/openai_api_key.txt
  slack_alert_webhook:
    file: ./secrets/slack_alert_webhook.txt
  pagerduty_routing_key:
    file: ./secrets/pagerduty_routing_key.txt
  # PublicAI API key for external models
  publicai_api_key:
    file: ./secrets/publicai_api_key.txt
