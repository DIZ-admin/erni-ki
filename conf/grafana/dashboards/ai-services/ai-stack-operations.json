{
  "uid": "ai-stack-ops",
  "title": "AI Stack Operations",
  "schemaVersion": 39,
  "version": 1,
  "refresh": "15s",
  "time": {"from": "now-1h", "to": "now"},
  "templating": {
    "list": [
      {
        "name": "model",
        "type": "textbox",
        "label": "Model",
        "query": "*",
        "current": {"text": "*", "value": "*"}
      }
    ]
  },
  "panels": [
    {
      "type": "timeseries",
      "title": "LiteLLM Requests",
      "id": 1,
      "gridPos": {"h": 6, "w": 12, "x": 0, "y": 0},
      "targets": [
        {"datasource": {"type": "prometheus", "uid": "prometheus"}, "expr": "sum(rate(litellm_proxy_requests_total{model=~\"$model\"}[5m]))", "legendFormat": "Requests", "refId": "A"}
      ]
    },
    {
      "type": "timeseries",
      "title": "LiteLLM Error Rate",
      "id": 2,
      "gridPos": {"h": 6, "w": 12, "x": 12, "y": 0},
      "targets": [
        {"datasource": {"type": "prometheus", "uid": "prometheus"}, "expr": "sum(rate(litellm_proxy_requests_failed_total[5m])) / sum(rate(litellm_proxy_requests_total[5m]))", "legendFormat": "Failure %", "refId": "A"}
      ],
      "fieldConfig": {"defaults": {"unit": "percent"}}
    },
    {
      "type": "timeseries",
      "title": "Ollama GPU Utilization",
      "id": 3,
      "gridPos": {"h": 6, "w": 8, "x": 0, "y": 6},
      "targets": [
        {"datasource": {"type": "prometheus", "uid": "prometheus"}, "expr": "avg(nvidia_smi_gpu_utilization{job=\"nvidia-exporter\"})", "legendFormat": "GPU", "refId": "A"}
      ],
      "fieldConfig": {"defaults": {"unit": "percent"}}
    },
    {
      "type": "timeseries",
      "title": "Docling Job Duration",
      "id": 4,
      "gridPos": {"h": 6, "w": 8, "x": 8, "y": 6},
      "targets": [
        {"datasource": {"type": "prometheus", "uid": "prometheus"}, "expr": "histogram_quantile(0.9, sum by (le) (rate(docling_pipeline_duration_seconds_bucket[5m])))", "legendFormat": "p90", "refId": "A"}
      ],
      "fieldConfig": {"defaults": {"unit": "s"}}
    },
    {
      "type": "timeSeries",
      "title": "OpenWebUI Latency",
      "id": 5,
      "gridPos": {"h": 6, "w": 8, "x": 16, "y": 6},
      "targets": [
        {"datasource": {"type": "prometheus", "uid": "prometheus"}, "expr": "histogram_quantile(0.95, sum by (le) (rate(openwebui_request_duration_seconds_bucket[5m])))", "legendFormat": "p95", "refId": "A"}
      ],
      "fieldConfig": {"defaults": {"unit": "s"}}
    },
    {
      "type": "stat",
      "title": "Queue Depth",
      "id": 6,
      "gridPos": {"h": 4, "w": 6, "x": 0, "y": 12},
      "targets": [{"datasource": {"type": "prometheus", "uid": "prometheus"}, "expr": "avg(litellm_queue_depth)", "refId": "A"}]
    },
    {
      "type": "table",
      "title": "Slow Requests (>5s)",
      "id": 7,
      "gridPos": {"h": 8, "w": 12, "x": 6, "y": 12},
      "targets": [
        {"datasource": {"type": "loki", "uid": "loki"}, "expr": "{container_name=\"litellm\"} |= \"duration\" |= \"ms\"", "refId": "A"}
      ],
      "options": {"showHeader": true}
    }
  ]
}
