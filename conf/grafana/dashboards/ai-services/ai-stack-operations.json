{
  "uid": "ai-stack-ops",
  "title": "AI Stack Operations",
  "schemaVersion": 39,
  "version": 2,
  "refresh": "30s",
  "time": {"from": "now-3h", "to": "now"},
  "panels": [
    {
      "type": "timeseries",
      "title": "Frontend Request Rate",
      "id": 1,
      "gridPos": {"h": 6, "w": 12, "x": 0, "y": 0},
      "targets": [
        {
          "datasource": {"type": "prometheus", "uid": "prometheus"},
          "expr": "sum(rate(nginx_http_requests_total[5m]))",
          "legendFormat": "req/s",
          "refId": "A"
        }
      ],
      "fieldConfig": {"defaults": {"unit": "req/s"}}
    },
    {
      "type": "timeseries",
      "title": "HTTP 5xx Ratio",
      "id": 2,
      "gridPos": {"h": 6, "w": 12, "x": 12, "y": 0},
      "targets": [
        {
          "datasource": {"type": "prometheus", "uid": "prometheus"},
          "expr": "sum(rate(nginx_http_requests_total{status=~\"5..\"}[5m])) / clamp_min(sum(rate(nginx_http_requests_total[5m])), 1)",
          "legendFormat": "5xx",
          "refId": "A"
        }
      ],
      "fieldConfig": {"defaults": {"unit": "percent", "thresholds": {"mode": "percentage", "steps": [{"color": "green", "value": 0}, {"color": "yellow", "value": 0.01}, {"color": "red", "value": 0.02}]}}}
    },
    {
      "type": "stat",
      "title": "Ollama Request Latency",
      "id": 3,
      "gridPos": {"h": 4, "w": 6, "x": 0, "y": 6},
      "targets": [
        {"datasource": {"type": "prometheus", "uid": "prometheus"}, "expr": "ollama_request_latency_seconds", "refId": "A"}
      ],
      "fieldConfig": {"defaults": {"unit": "s", "thresholds": {"mode": "absolute", "steps": [{"color": "green", "value": 0}, {"color": "yellow", "value": 1}, {"color": "red", "value": 3}]}}},
      "options": {"reduceOptions": {"calcs": ["lastNotNull"], "values": false}}
    },
    {
      "type": "stat",
      "title": "Ollama Installed Models",
      "id": 4,
      "gridPos": {"h": 4, "w": 6, "x": 6, "y": 6},
      "targets": [
        {"datasource": {"type": "prometheus", "uid": "prometheus"}, "expr": "ollama_installed_models", "refId": "A"}
      ],
      "fieldConfig": {"defaults": {"unit": "short"}}
    },
    {
      "type": "timeseries",
      "title": "GPU Temperature",
      "id": 5,
      "gridPos": {"h": 6, "w": 6, "x": 12, "y": 6},
      "targets": [
        {"datasource": {"type": "prometheus", "uid": "prometheus"}, "expr": "avg(nvidia_gpu_temperature_celsius)", "legendFormat": "GPU", "refId": "A"}
      ],
      "fieldConfig": {"defaults": {"unit": "celsius"}}
    },
    {
      "type": "timeseries",
      "title": "Redis Command Throughput",
      "id": 6,
      "gridPos": {"h": 6, "w": 9, "x": 18, "y": 6},
      "targets": [
        {"datasource": {"type": "prometheus", "uid": "prometheus"}, "expr": "sum(rate(redis_commands_processed_total[5m]))", "legendFormat": "cmd/s", "refId": "A"}
      ],
      "fieldConfig": {"defaults": {"unit": "req/s"}}
    },
    {
      "type": "timeseries",
      "title": "Redis Cache Hit Ratio",
      "id": 7,
      "gridPos": {"h": 6, "w": 12, "x": 0, "y": 10},
      "targets": [
        {"datasource": {"type": "prometheus", "uid": "prometheus"}, "expr": "sum(rate(redis_keyspace_hits_total[5m])) / clamp_min(sum(rate(redis_keyspace_hits_total[5m])) + sum(rate(redis_keyspace_misses_total[5m])), 1)", "refId": "A"}
      ],
      "fieldConfig": {"defaults": {"unit": "percent"}}
    },
    {
      "type": "logs",
      "title": "OpenWebUI & LiteLLM Logs",
      "id": 8,
      "gridPos": {"h": 10, "w": 24, "x": 0, "y": 16},
      "datasource": {"type": "loki", "uid": "loki"},
      "targets": [
        {"datasource": {"type": "loki", "uid": "loki"}, "expr": "{container_name=~\"openwebui|litellm\"} |= \"ERROR\"", "refId": "A"}
      ],
      "options": {"showLabels": true, "wrapLogMessage": true}
    }
  ]
}
