# LiteLLM Simple Configuration для ERNI-KI
# Минимальная конфигурация для работы с Ollama

# === MODEL LIST CONFIGURATION ===
model_list:
  # Local Ollama models
  - model_name: local-phi4-mini
    litellm_params:
      model: ollama/phi4-mini-reasoning:3.8b
      api_base: http://ollama:11434
      rpm: 1000
      tpm: 50000
    model_info:
      mode: chat
      supports_function_calling: true
      supports_vision: false
      context_window: 8192
      max_output_tokens: 4096

  - model_name: local-deepseek-r1
    litellm_params:
      model: ollama/deepseek-r1:7b
      api_base: http://ollama:11434
      rpm: 800
      tpm: 40000
    model_info:
      mode: chat
      supports_function_calling: true
      supports_vision: false
      context_window: 16384
      max_output_tokens: 8192

  - model_name: local-gemma3n
    litellm_params:
      model: ollama/gemma3n:e4b
      api_base: http://ollama:11434
      rpm: 600
      tpm: 30000
    model_info:
      mode: chat
      supports_function_calling: true
      supports_vision: false
      context_window: 8192
      max_output_tokens: 4096

# === ROUTER SETTINGS ===
router_settings:
  num_retries: 3
  timeout: 600

# === GENERAL SETTINGS ===
general_settings:
  master_key: os.environ/LITELLM_MASTER_KEY
  database_url: os.environ/DATABASE_URL

# === LITELLM SETTINGS ===
litellm_settings:
  drop_params: true
  set_verbose: false
  num_retries: 3
  request_timeout: 600
