# LiteLLM Unified Configuration for ERNI-KI
# Single proxy for all AI models with OpenAI API and local Ollama integration
#
# NOTE: Environment variable syntax "os.environ/VAR_NAME" is LiteLLM-specific.
# See: https://docs.litellm.ai/docs/proxy/configs#environment-variables
#
# Models are managed via Database Models in the LiteLLM Admin UI
# All models are migrated into the database for full control via the web UI
#
# === vLLM INTEGRATION CONFIGURATION ===
# To add vLLM models via Admin UI use:
#
# vLLM Model Configuration (add via LiteLLM Admin UI):
# - model_name: "vllm/llama-3.1-8b-instruct"
# - litellm_params:
#     model: "meta-llama/Llama-3.1-8B-Instruct"
#     api_base: "http://vllm:8000/v1"
#     api_key: "your-vllm-api-key"
#     custom_llm_provider: "openai"
#     max_tokens: 4096
#     temperature: 0.7
#
# Fallback Configuration (configure via Admin UI):
# - Primary: vllm/llama-3.1-8b-instruct (high performance)
# - Fallback: ollama/llama3.1:8b (compatibility)
#



router_settings:
  num_retries: 3
  timeout: 600
  routing_strategy: "usage-based-routing-v2"
  fallback_models: [] # Fallback models will be configured via Database Models
  enable_fallbacks: true # Enable fallback for production
  cooldown_time: 30 # Cooldown before retry (seconds)
  allowed_fails: 3 # Number of failures before fallback
  # === CONTEXT WINDOW FALLBACK CONFIGURATION ===
  # Auto-switch to larger context models when exceeding limits
  context_window_fallbacks: []
  # Redis settings for distributed routing (enabled in v1.80.0-stable.1)
  redis_host: "redis"
  redis_port: 6379
  redis_password: os.environ/REDIS_PASSWORD
  redis_db: 1

general_settings:
  master_key: os.environ/LITELLM_MASTER_KEY
  database_url: os.environ/DATABASE_URL  # set via environment variable
  ui_username: os.environ/UI_USERNAME
  ui_password: os.environ/UI_PASSWORD
  max_budget: 100
  # === DATABASE STORAGE SETTINGS ===
  # Models stored in database for persistence and Admin UI management
  # Synced with STORE_MODEL_IN_DB env variable
  store_model_in_db: true
  # Store prompts and responses in spend logs (detailed logging)
  store_prompts_in_spend_logs: false
  # Persist all request data
  store_audit_logs_in_db: false
  # Redact API key info in logs (enabled for production security)
  redact_user_api_key_info: true
  # Additional prompt logging settings
  # Synced with DISABLE_SPEND_LOGS env variable (False = enabled)
  disable_spend_logs: false
  disable_error_logs: false
  # === DETAILED LOGGING SETTINGS (hardened for prod) ===
  # Note: log_raw_request/log_raw_response moved to litellm_settings section
  detailed_debug: false
  # === SECURITY SETTINGS ===
  # Enforce auth for all /v1/* endpoints (disabled for OpenWebUI compatibility)
  enforce_user_param: false
  # Allow authenticated requests only
  # Enable audit logs for security
  enable_audit_logs: true
  # === HEALTH CHECK BYPASS ===
  # Disable auth for health check endpoints
  disable_auth_on_health_check: true
  # Disable logging of health checks to reduce noise
  disable_health_check_logs: true

# === OPENAI PASSTHROUGH CONFIGURATION (RESERVED) ===
# Uncomment when OpenAI Assistant API passthrough is needed
# openai_route_config:
#   enable_openai_passthrough: true
#   openai_api_key: os.environ/OPENAI_API_KEY
#   openai_api_base: os.environ/OPENAI_API_BASE
#   allowed_routes:
#     - "/v1/assistants"
#     - "/v1/assistants/*"
#     - "/v1/threads"
#     - "/v1/threads/*"
#   require_auth: true

litellm_settings:
  drop_params: false  # Keep params but avoid raw bodies
  num_retries: 3
  request_timeout: 300
  max_parallel_requests: 20  # Limit concurrent requests to reduce memory peaks
  custom_provider_map:
    - provider: "publicai"
      custom_handler: custom_providers.publicai.publicai_handler
  # === CONTEXT WINDOW MANAGEMENT ===
  # Auto-truncate context when exceeding model limits
  enable_context_window_fallback: true
  # Truncation strategy: "truncate" (drop oldest) or "summarize"
  context_window_fallback_strategy: "truncate"
  # Reserve 10% of context for the model response
  context_window_safety_margin: 0.1
  # Default context window size (32K for most models)
  max_context_window: 32768
  # === DETAILED LOGGING SETTINGS ===
  log_raw_request: false
  log_raw_response: false
  # Store prompts in logs
  store_prompts_in_logs: false
  # Alias map for external models to keep friendly names in OpenWebUI.
  model_alias_map:
    apertus-70b-instruct: "publicai/apertus-70b-instruct"
  # === PROMETHEUS METRICS ===
  # Disabled: requires Enterprise license
  # callbacks: ["prometheus"]  # Enable Prometheus metrics export
  # service_callback: ["prometheus_system"]  # Monitor system services (Redis, PostgreSQL)
  # === CACHE CONFIGURATION ===
  cache: true
  # === DETAILED LOG SETTINGS ===
  # Note: disable_health_check_logs is in general_settings section
  # Log all requests for detailed debugging
  log_only_errors: false
  # Detailed logging level - synced with LITELLM_LOG env (INFO for production)
  log_level: "INFO"
  # === OLLAMA THINKING TOKENS COMPATIBILITY ===
  # Settings for correct handling of thinking tokens
  ignore_unknown_fields: true
  ollama_ignore_thinking: true
  # Enable thinking token support in streaming
  enable_thinking_tokens: true
  # Treat thinking as part of the response
  thinking_as_response: true
  # Ollama provider settings
  ollama_thinking_mode: "stream"
  # Allow unknown fields in chunk responses
  allow_unknown_chunk_fields: true
  # === vLLM INTEGRATION SETTINGS (RESERVED) ===
  # Uncomment when vLLM server is deployed
  # See docs/examples/litellm-config.yaml for vLLM model configuration via Admin UI
  # vllm_compatibility_mode: true
  # vllm_enable_prefix_caching: true
  # vllm_request_timeout: 600
  # vllm_enable_streaming: true
  # === MCP INTEGRATION SETTINGS (RESERVED) ===
  # MCP server integration for future use
  # Requires LiteLLM Enterprise or compatible setup
  # mcp_aliases:
  #   "deepwiki": "deepwiki_mcp"
  # === KEY GENERATION SETTINGS ===
  # Default key generation settings
  default_key_generate_params:
    max_budget: 10.0 # Default budget $10
    duration: "30d" # Key validity 30 days
    metadata:
      created_by: "erni-ki-system"
      environment: "production"
  # Key generation limits
  upperbound_key_generate_params:
    max_budget: 100.0 # Max budget $100
    duration: "90d" # Max validity 90 days
    max_parallel_requests: 50 # Max parallel requests
    tpm_limit: 10000 # Tokens per minute limit
    rpm_limit: 100 # Requests per minute limit
  # === PERFORMANCE OPTIMIZATION ===
  # Redis caching enabled (v1.80.0-stable.1 fixes socket_timeout bug)
  # Password is taken from REDIS_URL env (set by entrypoint from docker secret)
  cache_params:
    type: "redis"
    url: os.environ/REDIS_URL
    ttl: 1800 # Cache TTL in seconds (30 minutes)
    supported_call_types: ["acompletion", "atext_completion", "aembedding", "atranscription"]
    # Timeout settings (increased for stability)
    socket_connect_timeout: 10
    socket_timeout: 30
    connection_pool_timeout: 5
    retry_on_timeout: true
    health_check_interval: 30
  # Connection pooling
  connection_pool:
    max_connections: 100 # Max DB connections
    max_overflow: 20 # Extra connections during peak
    pool_timeout: 30 # Connection acquire timeout
    pool_recycle: 3600 # Connection lifetime (1 hour)

# === OPENAI ASSISTANT API CONFIGURATION (RESERVED) ===
# Uncomment when OpenAI Assistant API passthrough is needed
# assistant_settings:
#   custom_llm_provider: openai
#   litellm_params:
#     api_key: os.environ/OPENAI_API_KEY
#     api_base: os.environ/OPENAI_API_BASE
#   default_assistant_id: "asst_YOUR_ASSISTANT_ID"
#   enable_assistant_passthrough: true

# === MCP SERVERS CONFIGURATION (RESERVED) ===
# Example configuration for external MCP servers
# Uncomment when MCP integration is required
# mcp_servers:
#   deepwiki_mcp:
#     url: "https://mcp.deepwiki.com/mcp"
#     transport: "http"
#     description: "DeepWiki MCP Server"
#     auth_type: "none"
