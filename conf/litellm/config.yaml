# LiteLLM Unified Configuration for ERNI-KI
# Single proxy for all AI models with OpenAI API and local Ollama integration

# Models are managed via Database Models in the LiteLLM Admin UI
# All models are migrated into the database for full control via the web UI
#
# === vLLM INTEGRATION CONFIGURATION ===
# To add vLLM models via Admin UI use:
#
# vLLM Model Configuration (add via LiteLLM Admin UI):
# - model_name: "vllm/llama-3.1-8b-instruct"
# - litellm_params:
#     model: "meta-llama/Llama-3.1-8B-Instruct"
#     api_base: "http://vllm:8000/v1"
#     api_key: "erni-ki-vllm-secure-key-2024"
#     custom_llm_provider: "openai"
#     max_tokens: 4096
#     temperature: 0.7
#
# Fallback Configuration (configure via Admin UI):
# - Primary: vllm/llama-3.1-8b-instruct (high performance)
# - Fallback: ollama/llama3.1:8b (compatibility)
#



router_settings:
  num_retries: 3
  timeout: 600
  routing_strategy: "usage-based-routing-v2"
  fallback_models: [] # Fallback models will be configured via Database Models
  enable_fallbacks: true # Enable fallback for production
  cooldown_time: 30 # Cooldown before retry (seconds)
  allowed_fails: 3 # Number of failures before fallback
  # === CONTEXT WINDOW FALLBACK CONFIGURATION ===
  # Auto-switch to larger context models when exceeding limits
  context_window_fallbacks: []
  # Redis settings for router are temporarily disabled due to incompatibility
  # redis_host: "redis"
  # redis_port: 6379
  # redis_password: "ErniKiRedisSecurePassword2024"
  # redis_db: 1 # Use the same DB as caching

general_settings:
  master_key: os.environ/LITELLM_MASTER_KEY
  database_url: os.environ/DATABASE_URL
  ui_username: os.environ/UI_USERNAME
  ui_password: os.environ/UI_PASSWORD
  max_budget: 100
  # === DATABASE STORAGE SETTINGS ===
  # Using Database Models â€” all models are added via Admin UI/API.
  store_model_in_db: true
  # Store prompts and responses in spend logs (detailed logging)
  store_prompts_in_spend_logs: true
  # Persist all request data
  store_audit_logs_in_db: true
  # Disable sensitive data filtering for debugging
  redact_user_api_key_info: false
  # Additional prompt logging settings
  disable_spend_logs: false
  disable_error_logs: false
  # === DETAILED LOGGING SETTINGS ===
  # Enable detailed request/response logging
  detailed_debug: true
  # Log all request parameters
  log_raw_request: true
  # Log full responses
  log_raw_response: true
  # === SECURITY SETTINGS ===
  # Enforce auth for all /v1/* endpoints (disabled for OpenWebUI compatibility)
  enforce_user_param: false
  # Allow authenticated requests only
  # Enable audit logs for security
  enable_audit_logs: true
  # === HEALTH CHECK BYPASS ===
  # Disable auth for health check endpoints
  disable_auth_on_health_check: true
  # Disable logging of health checks to reduce noise
  disable_health_check_logs: true

# === OPENAI PASSTHROUGH CONFIGURATION ===
# Passthrough for OpenAI Assistant API (updated 2025-10-07)
openai_route_config:
  # Enable OpenAI passthrough
  enable_openai_passthrough: true
  # OpenAI API settings
  openai_api_key: os.environ/OPENAI_API_KEY
  openai_api_base: os.environ/OPENAI_API_BASE
  # Allowed endpoints for passthrough
  allowed_routes:
    - "/v1/assistants"
    - "/v1/assistants/*"
    - "/v1/threads"
    - "/v1/threads/*"
    - "/v1/threads/*/messages"
    - "/v1/threads/*/messages/*"
    - "/v1/threads/*/runs"
    - "/v1/threads/*/runs/*"
  # Security settings for passthrough
  require_auth: true
  budget_alerts: false
  fallbacks: false

litellm_settings:
  drop_params: false  # Do not drop params; keep detailed logging
  # set_verbose: true   # Deprecated; use LITELLM_LOG=DEBUG in env
  num_retries: 3
  request_timeout: 300
  max_parallel_requests: 20  # Limit concurrent requests to reduce memory peaks
  custom_provider_map:
    - provider: "publicai"
      custom_handler: custom_providers.publicai.publicai_handler
  # === CONTEXT WINDOW MANAGEMENT ===
  # Auto-truncate context when exceeding model limits
  enable_context_window_fallback: true
  # Truncation strategy: "truncate" (drop oldest) or "summarize"
  context_window_fallback_strategy: "truncate"
  # Reserve 10% of context for the model response
  context_window_safety_margin: 0.1
  # Default context window size (32K for most models)
  max_context_window: 32768
  # === DETAILED LOGGING SETTINGS ===
  # Enable detailed logging of requests/responses
  log_raw_request: true
  log_raw_response: true
  # Store prompts in logs
  store_prompts_in_logs: true
  # Alias map for external models to keep friendly names in OpenWebUI.
  model_alias_map:
    apertus-70b-instruct: "publicai/apertus-70b-instruct"
  # === PROMETHEUS METRICS ===
  # Disabled: requires Enterprise license
  # callbacks: ["prometheus"]  # Enable Prometheus metrics export
  # service_callback: ["prometheus_system"]  # Monitor system services (Redis, PostgreSQL)
  # === CACHE CONFIGURATION ===
  # Enabled: 2025-10-02 after Redis config fix
  cache: true  # Enable caching
  # === DETAILED LOG SETTINGS ===
  # Disable health check logging to reduce noise
  disable_health_check_logs: true
  # Log all requests for detailed debugging
  log_only_errors: false
  # Detailed logging level
  log_level: "DEBUG"
  # === OLLAMA THINKING TOKENS COMPATIBILITY ===
  # Settings for correct handling of thinking tokens
  ignore_unknown_fields: true
  ollama_ignore_thinking: true
  # Enable thinking token support in streaming
  enable_thinking_tokens: true
  # Treat thinking as part of the response
  thinking_as_response: true
  # Ollama provider settings
  ollama_thinking_mode: "stream"
  # Allow unknown fields in chunk responses
  allow_unknown_chunk_fields: true
  # === vLLM INTEGRATION SETTINGS ===
  # vLLM server integration settings
  vllm_compatibility_mode: true
  # Support vLLM-specific parameters
  vllm_enable_prefix_caching: true
  # Timeout for vLLM requests (higher for large models)
  vllm_request_timeout: 600
  # Streaming support for vLLM
  vllm_enable_streaming: true
  # === MCP INTEGRATION SETTINGS ===
  # MCP Aliases for simpler server access
  mcp_aliases:
    "deepwiki": "deepwiki_mcp"
  # === KEY GENERATION SETTINGS ===
  # Default key generation settings
  default_key_generate_params:
    max_budget: 10.0 # Default budget $10
    duration: "30d" # Key validity 30 days
    metadata:
      created_by: "erni-ki-system"
      environment: "production"
  # Key generation limits
  upperbound_key_generate_params:
    max_budget: 100.0 # Max budget $100
    duration: "90d" # Max validity 90 days
    max_parallel_requests: 50 # Max parallel requests
    tpm_limit: 10000 # Tokens per minute limit
    rpm_limit: 100 # Requests per minute limit
  # === PERFORMANCE OPTIMIZATION ===
  # Caching settings for production
  # Enabled: 2025-10-02 using in-memory caching
  # Redis caching has a bug in LiteLLM v1.80.0.rc.1 (socket_timeout: 5.0 hardcoded)
  cache_params:
    type: "local" # Use in-memory caching
    ttl: 1800 # Cache TTL in seconds (30 minutes)
    supported_call_types: ["acompletion", "atext_completion", "aembedding", "atranscription"]
  #   socket_connect_timeout: 10 # Connection timeout
  #   socket_timeout: 10 # Operation timeout
  #   connection_pool_timeout: 5 # Connection pool timeout
  #   retry_on_timeout: true # Retry on timeout
  #   health_check_interval: 30 # Health check interval
  # Connection pooling
  connection_pool:
    max_connections: 100 # Max DB connections
    max_overflow: 20 # Extra connections during peak
    pool_timeout: 30 # Connection acquire timeout
    pool_recycle: 3600 # Connection lifetime (1 hour)

# === OPENAI ASSISTANT API CONFIGURATION ===
# Settings for OpenAI Assistant API integration (updated 2025-10-07)
assistant_settings:
  custom_llm_provider: openai
  litellm_params:
    api_key: os.environ/OPENAI_API_KEY
    api_base: os.environ/OPENAI_API_BASE
  # Specific Assistant settings
  default_assistant_id: "asst_C8dUl6EKuR41O9sddVVuhTGn"
  # Enable passthrough for Assistant API endpoints
  enable_assistant_passthrough: true
  # Supported Assistant API operations
  supported_operations:
    - "create_assistant"
    - "get_assistant"
    - "update_assistant"
    - "delete_assistant"
    - "create_thread"
    - "get_thread"
    - "update_thread"
    - "delete_thread"
    - "create_message"
    - "get_message"
    - "list_messages"
    - "create_run"
    - "get_run"
    - "list_runs"
    - "cancel_run"

# === MCP SERVERS CONFIGURATION ===
# Integration with external MCP servers (documentation examples)
mcp_servers:
  # Example external MCP server - DeepWiki
  deepwiki_mcp:
    url: "https://mcp.deepwiki.com/mcp"
    transport: "http"
    description: "DeepWiki MCP Server for search"
    auth_type: "none"
    spec_version: "2025-03-26"
    mcp_info:
      mcp_server_cost_info:
        default_cost_per_query: 0.001 # $0.001 per query
