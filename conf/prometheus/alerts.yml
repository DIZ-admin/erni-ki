# ERNI-KI Prometheus Alert Rules
# Date created: 2025-10-24
# Proactive monitoring of critical system metrics

groups:
  - name: erni-ki-critical-alerts
    interval: 30s
    rules:
      # === DISK SPACE ALERTS ===
      - alert: DiskSpaceCritical
        expr: (node_filesystem_avail_bytes{mountpoint="/"} / node_filesystem_size_bytes{mountpoint="/"}) < 0.15
        for: 5m
        labels:
          severity: critical
          component: infrastructure
        annotations:
          summary: "Critical low disk space"
          description: "Less than 15% disk space available ({{ $value | humanizePercentage }})"
          action: "Immediately clean Docker cache and old data"

      - alert: DiskSpaceWarning
        expr: (node_filesystem_avail_bytes{mountpoint="/"} / node_filesystem_size_bytes{mountpoint="/"}) < 0.25
        for: 10m
        labels:
          severity: warning
          component: infrastructure
        annotations:
          summary: "Low disk space"
          description: "Less than 25% disk space available ({{ $value | humanizePercentage }})"
          action: "Schedule Docker cache cleanup"

      # === MEMORY ALERTS ===
      - alert: MemoryCritical
        expr: (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes) < 0.05
        for: 5m
        labels:
          severity: critical
          component: infrastructure
        annotations:
          summary: "Critical low memory"
          description: "Less than 5% memory available ({{ $value | humanizePercentage }})"
          action: "Check for memory leaks, restart problematic containers"

      - alert: MemoryWarning
        expr: (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes) < 0.15
        for: 10m
        labels:
          severity: warning
          component: infrastructure
        annotations:
          summary: "Low available memory"
          description: "Less than 15% memory available ({{ $value | humanizePercentage }})"
          action: "Monitor container memory usage"

      # === CONTAINER HEALTH ALERTS ===
      - alert: ContainerDown
        expr: up{job=~".*"} == 0
        for: 2m
        labels:
          severity: critical
          component: "{{ $labels.job }}"
        annotations:
          summary: "Container {{ $labels.job }} is down"
          description: "Service {{ $labels.job }} has not responded for more than 2 minutes"
          action: "Check logs: docker compose logs {{ $labels.job }}"

      - alert: ContainerRestarting
        expr: |
          sum by (service) (
            label_replace(
              changes(
                container_start_time_seconds{
                  job="cadvisor",
                  name=~"[0-9a-f]{10,}_erni-ki-(redis|prometheus|alertmanager|fluent-bit|openwebui|litellm|nginx|ollama|db|grafana|searxng|watchtower|webhook-receiver|mcposerver).*"
                }[30m]
              ),
              "service",
              "$1",
              "name",
              "^[0-9a-f]{10,}_(erni-ki-[a-z0-9-]+)"
            )
          ) >= 3
        for: 5m
        labels:
          severity: warning
          component: "{{ $labels.service }}"
        annotations:
          summary: "Container {{ $labels.service }} restarting too often"
          description: "Service {{ $labels.service }} has restarted {{ printf \"%.0f\" $value }} times in the last 30 minutes"
          action: "Check docker logs {{ $labels.service }} and restart reasons"

      # === CPU ALERTS ===
      - alert: HighCPUUsage
        expr: (100 - (avg by (instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)) > 90
        for: 10m
        labels:
          severity: warning
          component: infrastructure
        annotations:
          summary: "High CPU usage"
          description: "CPU usage above 90% for 10 minutes ({{ $value }}%)"
          action: "Check top processes: docker stats"

      # === POSTGRESQL ALERTS ===
      - alert: PostgreSQLTooManyConnections
        expr: pg_stat_database_numbackends{datname="openwebui"} > 80
        for: 5m
        labels:
          severity: warning
          component: postgresql
        annotations:
          summary: "Too many PostgreSQL connections"
          description: "{{ $value }} active connections to openwebui DB"
          action: "Check connection pooling in OpenWebUI"

      - alert: PostgreSQLDown
        expr: pg_up == 0
        for: 1m
        labels:
          severity: critical
          component: postgresql
        annotations:
          summary: "PostgreSQL is down"
          description: "PostgreSQL has not responded for more than 1 minute"
          action: "Immediately check status: docker compose ps db"

      # === REDIS ALERTS ===
      - alert: RedisDown
        expr: redis_up == 0
        for: 1m
        labels:
          severity: critical
          component: redis
        annotations:
          summary: "Redis is down"
          description: "Redis has not responded for more than 1 minute"
          action: "Check status: docker compose ps redis"

      - alert: RedisLowHitRatio
        expr: (rate(redis_keyspace_hits_total[5m]) / (rate(redis_keyspace_hits_total[5m]) + rate(redis_keyspace_misses_total[5m]))) < 0.80 and (rate(redis_keyspace_hits_total[5m]) + rate(redis_keyspace_misses_total[5m])) > 0.1
        for: 15m
        labels:
          severity: warning
          component: redis
        annotations:
          summary: "Low Redis hit ratio"
          description: "Hit ratio {{ $value | humanizePercentage }} (normal >90%). Only triggers when request rate > 0.1/s."
          action: "Check key TTL and caching patterns"

      # === OLLAMA GPU ALERTS ===
      - alert: OllamaGPUMemoryHigh
        expr: (nvidia_gpu_memory_used_bytes / nvidia_gpu_memory_total_bytes) > 0.90
        for: 5m
        labels:
          severity: warning
          component: ollama
        annotations:
          summary: "High GPU memory usage"
          description: "VRAM usage above 90% ({{ $value | humanizePercentage }})"
          action: "Check loaded models: docker compose exec ollama ollama list"

      # === NGINX ALERTS ===
      - alert: NginxHighErrorRate
        expr: rate(nginx_http_requests_total{status=~"5.."}[5m]) > 10
        for: 5m
        labels:
          severity: warning
          component: nginx
        annotations:
          summary: "High Nginx 5xx error rate"
          description: "{{ $value }} 5xx errors per second"
          action: "Check upstream services and Nginx logs"

      # === PROMETHEUS SELF-MONITORING ===
      - alert: PrometheusTSDBCompactionsFailing
        expr: rate(prometheus_tsdb_compactions_failed_total[5m]) > 0
        for: 5m
        labels:
          severity: warning
          component: prometheus
        annotations:
          summary: "Prometheus TSDB compaction failures"
          description: "TSDB compaction is failing"
          action: "Check disk space and Prometheus logs"

      - alert: PrometheusTargetDown
        expr: up == 0
        for: 3m
        labels:
          severity: warning
          component: prometheus
        annotations:
          summary: "Prometheus target {{ $labels.job }} is down"
          description: "Target {{ $labels.job }} has not responded for more than 3 minutes"
          action: "Check service status: docker compose ps {{ $labels.job }}"

  - name: erni-ki-performance-alerts
    interval: 1m
    rules:
      # === OPENWEBUI PERFORMANCE ===
      - alert: OpenWebUISlowResponse
        expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket{job="openwebui"}[5m])) > 5
        for: 10m
        labels:
          severity: warning
          component: openwebui
        annotations:
          summary: "Slow OpenWebUI responses"
          description: "95th percentile response time > 5 seconds"
          action: "Check PostgreSQL and Ollama performance"

      # === SEARXNG PERFORMANCE ===
      - alert: SearXNGSlowSearch
        expr: histogram_quantile(0.95, rate(searxng_search_duration_seconds_bucket[5m])) > 3
        for: 10m
        labels:
          severity: warning
          component: searxng
        annotations:
          summary: "Slow SearXNG search"
          description: "95th percentile search time > 3 seconds"
          action: "Check search engines availability and Redis cache"

      # === DOCKER STORAGE ===
      - alert: DockerStoragePoolAlmostFull
        expr: (docker_data_space_used / docker_data_space_total) > 0.85
        for: 10m
        labels:
          severity: warning
          component: docker
        annotations:
          summary: "Docker storage pool almost full"
          description: "Used {{ $value | humanizePercentage }} of storage pool"
          action: "Run docker system prune"

      # === LITELLM PUBLICAI MONITORING ===
      - alert: LiteLLMPublicAIHighErrorRate
        expr: |
          sum(rate(litellm_publicai_requests_total{status=~"4..|5.."}[5m]))
          /
          clamp_min(sum(rate(litellm_publicai_requests_total[5m])), 1)
          > 0.1
        for: 2m
        labels:
          severity: warning
          component: litellm
        annotations:
          summary: "PublicAI error rate > 10%"
          description: "PublicAI 4xx/5xx response rate exceeded 10% in the last 5 minutes"
          action: "Check PublicAI key, rate limits, and litellm logs"

      - alert: LiteLLMPublicAIRepeated404
        expr: rate(litellm_publicai_requests_total{status="404"}[5m]) > 0.05
        for: 1m
        labels:
          severity: warning
          component: litellm
        annotations:
          summary: "PublicAI model missing"
          description: "liteLLM is receiving continuous 404 ('Model not found') from PublicAI"
          action: "Check that publicai-apertus-70b model is active and aliases are configured"

      # === LITELLM GATEWAY ALERTS ===
      - alert: LiteLLMGatewayDown
        expr: probe_success{job="blackbox", instance=~".*litellm.*"} == 0
        for: 2m
        labels:
          severity: critical
          component: litellm
        annotations:
          summary: "LiteLLM Gateway is down"
          description: "LiteLLM gateway has not responded to health checks for 2 minutes"
          action: "Check status: docker compose logs litellm"

      - alert: LiteLLMHighLatency
        expr: histogram_quantile(0.95, rate(litellm_request_duration_seconds_bucket[5m])) > 5
        for: 5m
        labels:
          severity: warning
          component: litellm
        annotations:
          summary: "High LiteLLM request latency"
          description: "95th percentile latency > 5 seconds ({{ $value | humanize }}s)"
          action: "Check upstream providers and network connectivity"

      - alert: LiteLLMBudgetExhausted
        expr: litellm_budget_remaining < 1
        for: 1m
        labels:
          severity: warning
          component: litellm
        annotations:
          summary: "LiteLLM budget nearly exhausted"
          description: "Less than $1 remaining in budget"
          action: "Review usage and increase budget if needed"
