# GPU Monitoring Alert Rules
# Covers: NVIDIA GPU metrics, temperature, utilization

groups:
  - name: gpu-monitoring.rules
    rules:
      - alert: GPUUnavailable
        expr: up{job="nvidia-exporter"} == 0
        for: 1m
        labels:
          severity: critical
          service: nvidia
          category: ai
          owner: ml-ops
          escalation: pagerduty
        annotations:
          summary: "GPU monitoring unavailable"
          description: "NVIDIA GPU exporter has been down for more than 1 minute."

      - alert: CriticalGPUUsage
        # Fixed: nvidia_gpu_duty_cycle instead of nvidia_gpu_utilization
        expr: nvidia_gpu_duty_cycle > 98
        for: 15m
        labels:
          severity: critical
          service: gpu
          category: ai
          owner: ml-ops
          escalation: pagerduty
        annotations:
          summary: "Critical GPU usage"
          description: "GPU utilization is above 98% for more than 15 minutes. Performance may be severely degraded."

      - alert: HighGPUMemoryUsage
        expr: (nvidia_gpu_memory_used_bytes / nvidia_gpu_memory_total_bytes) * 100 > 90
        for: 10m
        labels:
          severity: warning
          service: gpu
          category: ai
          owner: ml-ops
          escalation: slack
        annotations:
          summary: "High GPU memory usage"
          description: "GPU memory usage is above 90% for more than 10 minutes."

      - alert: CriticalGPUTemperature
        # Fixed: nvidia_gpu_temperature_celsius instead of nvidia_gpu_temperature
        expr: nvidia_gpu_temperature_celsius > 90
        for: 2m
        labels:
          severity: critical
          service: gpu
          category: hardware
          owner: ml-ops
          escalation: pagerduty
        annotations:
          summary: "Critical GPU temperature"
          description: "GPU temperature is above 90C for more than 2 minutes. Risk of hardware damage."

      - alert: LowGPUPerformance
        # Fixed: nvidia_gpu_power_usage_milliwatts instead of nvidia_gpu_power_draw
        # 50W = 50000mW, only alert when GPU is actually being used (duty_cycle > 5%)
        expr: nvidia_gpu_power_usage_milliwatts < 50000 and nvidia_gpu_duty_cycle > 5
        for: 30m
        labels:
          severity: warning
          service: gpu
          category: ai
          owner: ml-ops
          escalation: slack
        annotations:
          summary: "Low GPU performance"
          description: "GPU power draw is below 50W while GPU is active (>5% utilization) for more than 30 minutes. GPU may be throttling."
